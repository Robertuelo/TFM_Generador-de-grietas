{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xYbvevOpr4D9oc39nUgHe1bAS7rrOu_j","timestamp":1687592383100}],"gpuType":"T4","authorship_tag":"ABX9TyPQyOVan0oTU9i03OzHtymk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["https://github.com/ozanciga/gans-with-pytorch/tree/master/wgan-gp"],"metadata":{"id":"8iDPDfODsr0Y"}},{"cell_type":"code","source":["#torch cuda\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"o3O6OVP0srQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coCS0m76swXn","executionInfo":{"status":"ok","timestamp":1688228123717,"user_tz":-120,"elapsed":16625,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}},"outputId":"81d94c6f-5a98-48a9-c133-cb1c6c9eb7be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import cv2\n","import numpy as np"],"metadata":{"id":"Xu5UYC9IszG0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UAU_eyz3q6Bz"},"outputs":[],"source":["from torch import nn\n","\n","# Residual network.\n","# WGAN-GP paper defines a residual block with up & downsampling.\n","# See the official implementation (given in the paper).\n","# I use architectures described in the official implementation,\n","# since I find it hard to deduce the blocks given here from the text alone.\n","class MeanPoolConv(nn.Module):\n","    def __init__(self, n_input, n_output, k_size):\n","        super(MeanPoolConv, self).__init__()\n","        conv1 = nn.Conv2d(n_input, n_output, k_size, stride=1, padding=(k_size-1)//2, bias=True)\n","        self.model = nn.Sequential(conv1)\n","    def forward(self, x):\n","        out = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4.0\n","        out = self.model(out)\n","        return out\n","\n","class ConvMeanPool(nn.Module):\n","    def __init__(self, n_input, n_output, k_size):\n","        super(ConvMeanPool, self).__init__()\n","        conv1 = nn.Conv2d(n_input, n_output, k_size, stride=1, padding=(k_size-1)//2, bias=True)\n","        self.model = nn.Sequential(conv1)\n","    def forward(self, x):\n","        out = self.model(x)\n","        out = (out[:,:,::2,::2] + out[:,:,1::2,::2] + out[:,:,::2,1::2] + out[:,:,1::2,1::2]) / 4.0\n","        return out\n","\n","class UpsampleConv(nn.Module):\n","    def __init__(self, n_input, n_output, k_size):\n","        super(UpsampleConv, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.PixelShuffle(2),\n","            nn.Conv2d(n_input, n_output, k_size, stride=1, padding=(k_size-1)//2, bias=True)\n","        )\n","    def forward(self, x):\n","        x = x.repeat((1, 4, 1, 1)) # Weird concat of WGAN-GPs upsampling process.\n","        out = self.model(x)\n","        return out\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, n_input, n_output, k_size, resample='up', bn=True, spatial_dim=None):\n","        super(ResidualBlock, self).__init__()\n","\n","        self.resample = resample\n","\n","        if resample == 'up':\n","            self.conv1 = UpsampleConv(n_input, n_output, k_size)\n","            self.conv2 = nn.Conv2d(n_output, n_output, k_size, padding=(k_size-1)//2)\n","            self.conv_shortcut = UpsampleConv(n_input, n_output, k_size)\n","            self.out_dim = n_output\n","        elif resample == 'down':\n","            self.conv1 = nn.Conv2d(n_input, n_input, k_size, padding=(k_size-1)//2)\n","            self.conv2 = ConvMeanPool(n_input, n_output, k_size)\n","            self.conv_shortcut = ConvMeanPool(n_input, n_output, k_size)\n","            self.out_dim = n_output\n","            self.ln_dims = [n_input, spatial_dim, spatial_dim] # Define the dimensions for layer normalization.\n","        else:\n","            self.conv1 = nn.Conv2d(n_input, n_input, k_size, padding=(k_size-1)//2)\n","            self.conv2 = nn.Conv2d(n_input, n_input, k_size, padding=(k_size-1)//2)\n","            self.conv_shortcut = None # Identity\n","            self.out_dim = n_input\n","            self.ln_dims = [n_input, spatial_dim, spatial_dim]\n","\n","        self.model = nn.Sequential(\n","            nn.BatchNorm2d(n_input) if bn else nn.LayerNorm(self.ln_dims),\n","            nn.ReLU(inplace=True),\n","            self.conv1,\n","            nn.BatchNorm2d(self.out_dim) if bn else nn.LayerNorm(self.ln_dims),\n","            nn.ReLU(inplace=True),\n","            self.conv2,\n","        )\n","\n","    def forward(self, x):\n","        if self.conv_shortcut is None:\n","            return x + self.model(x)\n","        else:\n","            return self.conv_shortcut(x) + self.model(x)\n","\n","class DiscBlock1(nn.Module):\n","    def __init__(self, n_output):\n","        super(DiscBlock1, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(1, n_output, 3, padding=(3-1)//2)\n","        self.conv2 = ConvMeanPool(n_output, n_output, 1)\n","        self.conv_shortcut = MeanPoolConv(1, n_output, 1)\n","\n","        self.model = nn.Sequential(\n","            self.conv1,\n","            nn.ReLU(inplace=True),\n","            self.conv2\n","        )\n","\n","    def forward(self, x):\n","        return self.conv_shortcut(x) + self.model(x)\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        self.model = nn.Sequential(                     # 128 x 1 x 1\n","            nn.ConvTranspose2d(128, 128, 4, 1, 0),      # 128 x 4 x 4\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 8 x 8\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 16 x 16\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 32 x 32\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 64 x 64\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 1, 3, padding=(3-1)//2),     # 1 x 64 x 64\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        n_output = 128\n","        '''\n","        This is a parameter but since we experiment with a single size\n","        of 3 x 32 x 32 images, it is hardcoded here.\n","        '''\n","\n","        self.DiscBlock1 = DiscBlock1(n_output)                      # 128 x 64 x 64\n","\n","        self.model = nn.Sequential(\n","            ResidualBlock(n_output, n_output, 3, resample='down', bn=False, spatial_dim=32),  # 128 x 32 x 32\n","            ResidualBlock(n_output, n_output, 3, resample='down', bn=False, spatial_dim=16),  # 128 x 16 x 16\n","            ResidualBlock(n_output, n_output, 3, resample=None, bn=False, spatial_dim=8),    # 128 x 8 x 8\n","            ResidualBlock(n_output, n_output, 3, resample=None, bn=False, spatial_dim=8),    # 128 x 8 x 8\n","            nn.ReLU(inplace=True),\n","        )\n","        self.l1 = nn.Sequential(nn.Linear(128, 1))                  # 128 x 1\n","\n","    def forward(self, x):\n","        # x = x.view(-1, 3, 32, 32)\n","        y = self.DiscBlock1(x)\n","        y = self.model(y)\n","        y = y.view(x.size(0), 128, -1)\n","        y = y.mean(dim=2)\n","        out = self.l1(y).unsqueeze_(1).unsqueeze_(2)\n","        return out\n","\n","\n"]},{"cell_type":"code","source":["import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","class CustomDataset(data.Dataset):\n","    def __init__(self, X_folder, y_folder, transform=None):\n","        self.X_folder = X_folder\n","        self.y_folder = y_folder\n","        self.transform = transform\n","\n","        # Obtener la lista de nombres de archivo en las carpetas\n","        self.X_filenames = [filename for filename in os.listdir(X_folder) if filename.endswith('.jpg')]\n","        self.y_filenames = [filename for filename in os.listdir(y_folder) if filename.endswith('.jpg')]\n","\n","    def __len__(self):\n","        return len(self.X_filenames)\n","\n","    def __getitem__(self, index):\n","        X_filename = self.X_filenames[index]\n","        y_filename = self.y_filenames[index]\n","\n","        if not X_filename.endswith(\".jpg\"):\n","            return self.__getitem__((index + 1) % len(self))\n","\n","        # Cargar las imágenes y las etiquetas\n","        X = Image.open(os.path.join(self.X_folder, X_filename))\n","        y = Image.open(os.path.join(self.y_folder, y_filename))\n","\n","        if self.transform:\n","            X = self.transform(X)\n","            y = self.transform(y)\n","\n","        return X, y\n","\n","    def get_images(self):\n","        images = []\n","        for X_filename in self.X_filenames:\n","            X = Image.open(os.path.join(self.X_folder, X_filename))#.convert(\"RGB\")\n","            #X = self.resizer(X)\n","            if self.transform:\n","                X = self.transform(X)\n","            images.append(X)\n","        return images\n","\n","    def get_labels(self):\n","        labels = []\n","        for y_filename in self.y_filenames:\n","            y = Image.open(os.path.join(self.y_folder, y_filename))#.convert(\"RGB\")\n","            #y = self.resizer(y)\n","            if self.transform:\n","                y = self.transform(y)\n","            labels.append(y)\n","        return labels\n"],"metadata":{"id":"Wf9iRqQjuEtZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn, optim\n","from torch.autograd.variable import Variable\n","\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","import torchvision.utils as vutils\n","\n","import errno\n","import matplotlib.pyplot as plt\n","\n","class Args:\n","    def __init__(self):\n","        self.n_epochs = 100\n","        self.batch_size = 64\n","        self.alpha = 0.0001\n","        self.b1 = 0.5\n","        self.b2 = 0.9\n","        self.n_critic = 5\n","        self.lambda_1 = 10\n","        self.img_size = 64\n","        self.channels = 1\n","        self.sample_interval = 256\n","opt = Args()\n","\n","\n","img_dims = (opt.channels, opt.img_size, opt.img_size)\n","n_features = opt.channels * opt.img_size * opt.img_size\n","\n","# TODO: Use some initialization in the future.\n","def init_weights(m):\n","    if type(m) == nn.ConvTranspose2d:\n","        torch.nn.init.kaiming_normal(m.weight, mode='fan_out', nonlinearity='relu')\n","    elif type(m) == nn.Conv2d:\n","        torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n","\n","# Definir las rutas de las carpetas de entrenamiento y prueba\n","trainX_folder = '/content/gdrive/MyDrive/Dev/AI_MsC/TFM/CRACK500/traindata/traindata/'\n","trainy_folder = '/content/gdrive/MyDrive/Dev/AI_MsC/TFM/CRACK500/valdata/valdata/'\n","\n","# Crear las transformaciones\n","transform = transforms.Compose([\n","    transforms.Resize((opt.img_size, opt.img_size)),\n","    transforms.Grayscale(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5), (0.5)),\n","])\n","\n","# Crear el dataset personalizado de entrenamiento\n","dataset_full = CustomDataset(trainX_folder, trainy_folder, transform=transform)\n","# Obtener xtrain e ytrain\n","dataset = dataset_full.get_images()\n","# Crear los dataloaders\n","batch_iterator = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=2)\n","\n","cuda = torch.cuda.is_available()\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","gan_loss = nn.BCELoss()\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","optimizer_D = optim.Adam(discriminator.parameters(), lr=opt.alpha, betas=(opt.b1, opt.b2))\n","optimizer_G = optim.Adam(generator.parameters(), lr=opt.alpha, betas=(opt.b1, opt.b2))\n","\n","# Loss record.\n","g_losses = []\n","d_losses = []\n","epochs = []\n","loss_legend = ['Discriminator', 'Generator']\n","\n","if cuda:\n","    generator = generator.cuda()\n","    discriminator = discriminator.cuda()\n","\n","noise_fixed = Variable(Tensor(25, 128, 1, 1).normal_(0, 1), requires_grad=False) # To track the progress of the GAN.\n","\n","#Generar carpetas para almacenar las imágenes creadas\n","try:\n","    os.mkdir('/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images')\n","    os.mkdir('/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/Images')\n","    os.mkdir('/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/Images/fake_samples')\n","    os.mkdir('/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/models')\n","except OSError as e:\n","    if e.errno != errno.EEXIST:\n","        raise\n","\n","# Inicializar listas para almacenar las pérdidas\n","loss_D_values = []\n","loss_G_values = []\n","\n","\n","for epoch in range(opt.n_epochs):\n","    print('Epoch {}'.format(epoch))\n","    for i, batch in enumerate(batch_iterator):\n","        # == Discriminator update == #\n","        for iter in range(opt.n_critic):\n","            # Sample real and fake images, using notation in paper.\n","            x = Variable(batch.type(Tensor))\n","            noise = Variable(Tensor(batch.size(0), 128, 1, 1).normal_(0, 1))\n","            x_tilde = Variable(generator(noise), requires_grad=True)\n","\n","            epsilon = Variable(Tensor(batch.size(0), 1, 1, 1).uniform_(0, 1))\n","\n","            x_hat = epsilon*x + (1 - epsilon)*x_tilde\n","            x_hat = torch.autograd.Variable(x_hat, requires_grad=True)\n","\n","            # Put the interpolated data through critic.\n","            dw_x = discriminator(x_hat)\n","            # A great exercise on learning how the autograd.grad works!\n","            grad_x = torch.autograd.grad(outputs=dw_x, inputs=x_hat,\n","                                         grad_outputs=Variable(Tensor(batch.size(0), 1, 1, 1).fill_(1.0), requires_grad=False),\n","                                         create_graph=True, retain_graph=True, only_inputs=True)\n","            grad_x = grad_x[0].view(batch.size(0), -1)\n","            grad_x = grad_x.norm(p=2, dim=1) # My naming is inaccurate, this is the 2-norm of grad(D_w(x_hat))\n","\n","            # Update discriminator (or critic, since we don't output probabilities anymore).\n","            optimizer_D.zero_grad()\n","\n","            # WGAN-GP loss, defined properly as a loss unlike the WGAN paper.\n","            d_loss = torch.mean(discriminator(x_tilde)) - torch.mean(discriminator(x)) + opt.lambda_1*torch.mean((grad_x - 1)**2)\n","\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","        # == Generator update == #\n","        noise = Variable(Tensor(batch.size(0), 128, 1, 1).normal_(0, 1))\n","        imgs_fake = generator(noise)\n","\n","        optimizer_G.zero_grad()\n","\n","        g_loss = -torch.mean(discriminator(imgs_fake))\n","\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        # Almacenar las pérdidas del generador y del discriminador\n","        loss_G_values.append(g_loss.item())\n","        loss_D_values.append(d_loss.item())\n","\n","        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch+1, 70, i+1, len(batch_iterator), d_loss.data, g_loss.data))\n","        if i % 100 == 0: # Every 100 steps:\n","            vutils.save_image(x, '/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/Images/real_samples.png', normalize = True) # We save the real images of the minibatch.\n","            fake = generator(noise) # We get our fake generated images.\n","            vutils.save_image(fake.detach(), f\"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/Images/fake_samples/fake_samples_epoch_{epoch:03d}.png\", normalize=True) # We also save the fake generated images of the minibatch.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdvQ52nirB2X","executionInfo":{"status":"ok","timestamp":1688229596148,"user_tz":-120,"elapsed":1442342,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}},"outputId":"22e340cf-5191-40ea-de97-2b67e71e4f47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0\n","[1/70][1/4] Loss_D: 6.0372 Loss_G: 0.3326\n","[1/70][2/4] Loss_D: -0.9678 Loss_G: 3.8274\n","[1/70][3/4] Loss_D: -2.0442 Loss_G: 4.4304\n","[1/70][4/4] Loss_D: -0.8725 Loss_G: 6.7641\n","Epoch 1\n","[2/70][1/4] Loss_D: -8.4216 Loss_G: 8.3230\n","[2/70][2/4] Loss_D: -13.6420 Loss_G: 9.4959\n","[2/70][3/4] Loss_D: -12.6190 Loss_G: 10.2726\n","[2/70][4/4] Loss_D: -7.0857 Loss_G: 12.2459\n","Epoch 2\n","[3/70][1/4] Loss_D: -5.6817 Loss_G: -3.3951\n","[3/70][2/4] Loss_D: -3.7719 Loss_G: 11.3280\n","[3/70][3/4] Loss_D: -5.8322 Loss_G: 9.8119\n","[3/70][4/4] Loss_D: -6.1838 Loss_G: 6.0592\n","Epoch 3\n","[4/70][1/4] Loss_D: 6.8228 Loss_G: -9.9798\n","[4/70][2/4] Loss_D: 8.0074 Loss_G: 20.8234\n","[4/70][3/4] Loss_D: -3.6932 Loss_G: -4.9937\n","[4/70][4/4] Loss_D: -2.7355 Loss_G: 6.6351\n","Epoch 4\n","[5/70][1/4] Loss_D: -5.4916 Loss_G: 8.3477\n","[5/70][2/4] Loss_D: -4.1370 Loss_G: 2.0617\n","[5/70][3/4] Loss_D: -1.3843 Loss_G: -13.1400\n","[5/70][4/4] Loss_D: 7.1904 Loss_G: -14.5079\n","Epoch 5\n","[6/70][1/4] Loss_D: -1.8751 Loss_G: 19.2626\n","[6/70][2/4] Loss_D: 4.5948 Loss_G: -13.7802\n","[6/70][3/4] Loss_D: 5.8585 Loss_G: -16.3150\n","[6/70][4/4] Loss_D: 7.7133 Loss_G: -16.9782\n","Epoch 6\n","[7/70][1/4] Loss_D: 7.7979 Loss_G: -15.9216\n","[7/70][2/4] Loss_D: 7.2972 Loss_G: -5.8740\n","[7/70][3/4] Loss_D: -9.2210 Loss_G: 9.7872\n","[7/70][4/4] Loss_D: -2.8838 Loss_G: 4.9439\n","Epoch 7\n","[8/70][1/4] Loss_D: 1.2447 Loss_G: 22.5538\n","[8/70][2/4] Loss_D: -9.4223 Loss_G: 15.5650\n","[8/70][3/4] Loss_D: -6.2935 Loss_G: 7.8273\n","[8/70][4/4] Loss_D: 0.0455 Loss_G: 4.2192\n","Epoch 8\n","[9/70][1/4] Loss_D: 4.0358 Loss_G: 5.6253\n","[9/70][2/4] Loss_D: 2.7832 Loss_G: 17.4401\n","[9/70][3/4] Loss_D: -12.8343 Loss_G: 18.3696\n","[9/70][4/4] Loss_D: -9.2116 Loss_G: 12.9572\n","Epoch 9\n","[10/70][1/4] Loss_D: -3.0442 Loss_G: 10.3500\n","[10/70][2/4] Loss_D: 0.6606 Loss_G: 6.1760\n","[10/70][3/4] Loss_D: -7.0766 Loss_G: 14.1164\n","[10/70][4/4] Loss_D: -7.1566 Loss_G: 14.6927\n","Epoch 10\n","[11/70][1/4] Loss_D: -2.4877 Loss_G: 13.3637\n","[11/70][2/4] Loss_D: -0.0856 Loss_G: 18.5842\n","[11/70][3/4] Loss_D: 2.0863 Loss_G: 26.2189\n","[11/70][4/4] Loss_D: -7.5663 Loss_G: -5.7822\n","Epoch 11\n","[12/70][1/4] Loss_D: -9.9123 Loss_G: 11.8294\n","[12/70][2/4] Loss_D: -6.0555 Loss_G: 6.6102\n","[12/70][3/4] Loss_D: -0.8296 Loss_G: 4.9657\n","[12/70][4/4] Loss_D: 3.8783 Loss_G: -2.2045\n","Epoch 12\n","[13/70][1/4] Loss_D: 5.2413 Loss_G: -18.4137\n","[13/70][2/4] Loss_D: -7.0745 Loss_G: 13.8639\n","[13/70][3/4] Loss_D: -12.3840 Loss_G: 18.6497\n","[13/70][4/4] Loss_D: -4.4136 Loss_G: 15.2100\n","Epoch 13\n","[14/70][1/4] Loss_D: -0.4929 Loss_G: 10.5311\n","[14/70][2/4] Loss_D: -4.3998 Loss_G: 24.9883\n","[14/70][3/4] Loss_D: -2.3618 Loss_G: 25.8059\n","[14/70][4/4] Loss_D: 1.8583 Loss_G: 31.7552\n","Epoch 14\n","[15/70][1/4] Loss_D: 5.8234 Loss_G: 41.2192\n","[15/70][2/4] Loss_D: 7.2449 Loss_G: 44.6590\n","[15/70][3/4] Loss_D: 8.1369 Loss_G: 47.1484\n","[15/70][4/4] Loss_D: -2.7270 Loss_G: 53.1026\n","Epoch 15\n","[16/70][1/4] Loss_D: -18.3408 Loss_G: 55.0971\n","[16/70][2/4] Loss_D: -14.5345 Loss_G: 32.3063\n","[16/70][3/4] Loss_D: -6.5387 Loss_G: 10.3303\n","[16/70][4/4] Loss_D: -0.5442 Loss_G: 17.1237\n","Epoch 16\n","[17/70][1/4] Loss_D: 3.8627 Loss_G: 24.9345\n","[17/70][2/4] Loss_D: 6.0340 Loss_G: 25.8157\n","[17/70][3/4] Loss_D: 3.3246 Loss_G: 17.4189\n","[17/70][4/4] Loss_D: -6.7106 Loss_G: 26.1989\n","Epoch 17\n","[18/70][1/4] Loss_D: -15.1161 Loss_G: 19.4246\n","[18/70][2/4] Loss_D: -13.6093 Loss_G: 8.7417\n","[18/70][3/4] Loss_D: -12.1377 Loss_G: 7.6658\n","[18/70][4/4] Loss_D: -13.0997 Loss_G: 6.7874\n","Epoch 18\n","[19/70][1/4] Loss_D: -19.1712 Loss_G: 23.2631\n","[19/70][2/4] Loss_D: -17.9947 Loss_G: 15.8367\n","[19/70][3/4] Loss_D: -15.1268 Loss_G: 17.7909\n","[19/70][4/4] Loss_D: -19.9706 Loss_G: 37.4929\n","Epoch 19\n","[20/70][1/4] Loss_D: -24.2437 Loss_G: 19.5976\n","[20/70][2/4] Loss_D: -19.2229 Loss_G: 13.4057\n","[20/70][3/4] Loss_D: -14.1438 Loss_G: 17.9118\n","[20/70][4/4] Loss_D: -15.2698 Loss_G: 18.9479\n","Epoch 20\n","[21/70][1/4] Loss_D: -14.7420 Loss_G: 11.6729\n","[21/70][2/4] Loss_D: -9.5684 Loss_G: 23.5705\n","[21/70][3/4] Loss_D: -9.6604 Loss_G: 10.2109\n","[21/70][4/4] Loss_D: -6.8778 Loss_G: 10.6549\n","Epoch 21\n","[22/70][1/4] Loss_D: -6.2759 Loss_G: 7.4710\n","[22/70][2/4] Loss_D: -4.9390 Loss_G: 8.6938\n","[22/70][3/4] Loss_D: -3.7192 Loss_G: 1.4903\n","[22/70][4/4] Loss_D: -4.1783 Loss_G: 16.4586\n","Epoch 22\n","[23/70][1/4] Loss_D: -3.5110 Loss_G: -11.7575\n","[23/70][2/4] Loss_D: -3.4785 Loss_G: -3.6262\n","[23/70][3/4] Loss_D: -4.6312 Loss_G: 9.5258\n","[23/70][4/4] Loss_D: -4.8950 Loss_G: 7.9811\n","Epoch 23\n","[24/70][1/4] Loss_D: -4.0144 Loss_G: 8.8911\n","[24/70][2/4] Loss_D: -2.8018 Loss_G: 5.0541\n","[24/70][3/4] Loss_D: -4.4131 Loss_G: 8.8971\n","[24/70][4/4] Loss_D: -0.8119 Loss_G: 25.2490\n","Epoch 24\n","[25/70][1/4] Loss_D: -4.6111 Loss_G: 10.7878\n","[25/70][2/4] Loss_D: -2.3347 Loss_G: 2.5922\n","[25/70][3/4] Loss_D: -1.6642 Loss_G: 2.2787\n","[25/70][4/4] Loss_D: -3.4663 Loss_G: 2.0440\n","Epoch 25\n","[26/70][1/4] Loss_D: -3.7556 Loss_G: 14.0814\n","[26/70][2/4] Loss_D: -3.3956 Loss_G: 12.7021\n","[26/70][3/4] Loss_D: -2.8474 Loss_G: 12.2446\n","[26/70][4/4] Loss_D: 1.3343 Loss_G: 11.8677\n","Epoch 26\n","[27/70][1/4] Loss_D: 3.3955 Loss_G: 15.3172\n","[27/70][2/4] Loss_D: -5.2817 Loss_G: 37.2803\n","[27/70][3/4] Loss_D: -5.6516 Loss_G: 31.5692\n","[27/70][4/4] Loss_D: -2.6630 Loss_G: 30.1681\n","Epoch 27\n","[28/70][1/4] Loss_D: -0.8414 Loss_G: 27.6543\n","[28/70][2/4] Loss_D: -0.9208 Loss_G: 17.2951\n","[28/70][3/4] Loss_D: -2.9058 Loss_G: 18.7791\n","[28/70][4/4] Loss_D: -2.1442 Loss_G: 19.1854\n","Epoch 28\n","[29/70][1/4] Loss_D: -3.2022 Loss_G: 5.6271\n","[29/70][2/4] Loss_D: -5.7678 Loss_G: 16.5297\n","[29/70][3/4] Loss_D: -3.2429 Loss_G: 16.3868\n","[29/70][4/4] Loss_D: -3.9336 Loss_G: 7.7898\n","Epoch 29\n","[30/70][1/4] Loss_D: -4.6054 Loss_G: 6.4282\n","[30/70][2/4] Loss_D: -4.6334 Loss_G: 18.3325\n","[30/70][3/4] Loss_D: -2.1642 Loss_G: 20.5158\n","[30/70][4/4] Loss_D: -1.8147 Loss_G: 20.4158\n","Epoch 30\n","[31/70][1/4] Loss_D: -2.6296 Loss_G: 16.3256\n","[31/70][2/4] Loss_D: -4.6395 Loss_G: 16.6962\n","[31/70][3/4] Loss_D: -4.9120 Loss_G: 17.4625\n","[31/70][4/4] Loss_D: -4.0105 Loss_G: 19.4682\n","Epoch 31\n","[32/70][1/4] Loss_D: -1.2912 Loss_G: 15.1021\n","[32/70][2/4] Loss_D: -1.8395 Loss_G: 12.2176\n","[32/70][3/4] Loss_D: -4.5819 Loss_G: 14.1090\n","[32/70][4/4] Loss_D: -3.0602 Loss_G: 10.6768\n","Epoch 32\n","[33/70][1/4] Loss_D: -3.4544 Loss_G: 7.3358\n","[33/70][2/4] Loss_D: -1.8685 Loss_G: 4.4893\n","[33/70][3/4] Loss_D: -2.3368 Loss_G: 7.3712\n","[33/70][4/4] Loss_D: -0.8629 Loss_G: 2.5339\n","Epoch 33\n","[34/70][1/4] Loss_D: -0.2536 Loss_G: -5.4240\n","[34/70][2/4] Loss_D: 0.1891 Loss_G: -11.5693\n","[34/70][3/4] Loss_D: 1.7389 Loss_G: -22.2424\n","[34/70][4/4] Loss_D: 3.5852 Loss_G: -24.6055\n","Epoch 34\n","[35/70][1/4] Loss_D: 4.7021 Loss_G: -23.5590\n","[35/70][2/4] Loss_D: -7.7015 Loss_G: -9.6919\n","[35/70][3/4] Loss_D: -4.9993 Loss_G: -3.5066\n","[35/70][4/4] Loss_D: -2.3174 Loss_G: -3.6787\n","Epoch 35\n","[36/70][1/4] Loss_D: 1.0399 Loss_G: -1.0077\n","[36/70][2/4] Loss_D: 3.2354 Loss_G: 2.3197\n","[36/70][3/4] Loss_D: -5.7546 Loss_G: 0.4149\n","[36/70][4/4] Loss_D: -3.2305 Loss_G: 1.9572\n","Epoch 36\n","[37/70][1/4] Loss_D: -1.0913 Loss_G: 3.5839\n","[37/70][2/4] Loss_D: 0.7574 Loss_G: 4.7149\n","[37/70][3/4] Loss_D: 1.8509 Loss_G: 9.1049\n","[37/70][4/4] Loss_D: 3.0322 Loss_G: -1.5530\n","Epoch 37\n","[38/70][1/4] Loss_D: -8.7711 Loss_G: 2.1873\n","[38/70][2/4] Loss_D: -7.6291 Loss_G: 3.9987\n","[38/70][3/4] Loss_D: -5.1572 Loss_G: 4.3496\n","[38/70][4/4] Loss_D: -1.9233 Loss_G: 2.2307\n","Epoch 38\n","[39/70][1/4] Loss_D: 0.5944 Loss_G: -2.9571\n","[39/70][2/4] Loss_D: 2.8204 Loss_G: -16.8438\n","[39/70][3/4] Loss_D: 4.6347 Loss_G: -31.8874\n","[39/70][4/4] Loss_D: 4.4031 Loss_G: -37.8429\n","Epoch 39\n","[40/70][1/4] Loss_D: 1.9602 Loss_G: -40.8023\n","[40/70][2/4] Loss_D: -9.2053 Loss_G: -33.4897\n","[40/70][3/4] Loss_D: -6.1905 Loss_G: -11.4435\n","[40/70][4/4] Loss_D: -3.8663 Loss_G: -3.1665\n","Epoch 40\n","[41/70][1/4] Loss_D: -2.5992 Loss_G: 2.9326\n","[41/70][2/4] Loss_D: 0.2877 Loss_G: 13.7762\n","[41/70][3/4] Loss_D: -2.6552 Loss_G: 14.7953\n","[41/70][4/4] Loss_D: -2.8378 Loss_G: 15.7399\n","Epoch 41\n","[42/70][1/4] Loss_D: -1.8146 Loss_G: 15.6161\n","[42/70][2/4] Loss_D: -2.9385 Loss_G: 11.0069\n","[42/70][3/4] Loss_D: -2.6643 Loss_G: 12.3377\n","[42/70][4/4] Loss_D: -2.7236 Loss_G: 5.2520\n","Epoch 42\n","[43/70][1/4] Loss_D: -1.8966 Loss_G: 4.4820\n","[43/70][2/4] Loss_D: -1.6280 Loss_G: 5.8806\n","[43/70][3/4] Loss_D: 0.0142 Loss_G: -5.0422\n","[43/70][4/4] Loss_D: -2.3015 Loss_G: 1.4712\n","Epoch 43\n","[44/70][1/4] Loss_D: -2.1912 Loss_G: 5.8164\n","[44/70][2/4] Loss_D: -1.0119 Loss_G: 10.2655\n","[44/70][3/4] Loss_D: -1.1780 Loss_G: 13.6229\n","[44/70][4/4] Loss_D: -1.3103 Loss_G: 20.5759\n","Epoch 44\n","[45/70][1/4] Loss_D: -2.4108 Loss_G: 5.4893\n","[45/70][2/4] Loss_D: -2.1668 Loss_G: 9.3586\n","[45/70][3/4] Loss_D: -1.9883 Loss_G: 16.7837\n","[45/70][4/4] Loss_D: -2.5981 Loss_G: 8.3972\n","Epoch 45\n","[46/70][1/4] Loss_D: -2.4054 Loss_G: 10.7731\n","[46/70][2/4] Loss_D: -0.9869 Loss_G: 20.2645\n","[46/70][3/4] Loss_D: -2.9411 Loss_G: 3.2310\n","[46/70][4/4] Loss_D: -3.6832 Loss_G: 8.5455\n","Epoch 46\n","[47/70][1/4] Loss_D: -2.6166 Loss_G: 1.4240\n","[47/70][2/4] Loss_D: -2.2774 Loss_G: 11.8298\n","[47/70][3/4] Loss_D: -1.6017 Loss_G: 8.7584\n","[47/70][4/4] Loss_D: -0.3321 Loss_G: 3.9963\n","Epoch 47\n","[48/70][1/4] Loss_D: -1.0962 Loss_G: -8.1585\n","[48/70][2/4] Loss_D: -0.2002 Loss_G: 4.2322\n","[48/70][3/4] Loss_D: -1.2735 Loss_G: 23.6551\n","[48/70][4/4] Loss_D: 1.4859 Loss_G: 24.8247\n","Epoch 48\n","[49/70][1/4] Loss_D: -0.6844 Loss_G: 25.9003\n","[49/70][2/4] Loss_D: -3.0817 Loss_G: 15.2222\n","[49/70][3/4] Loss_D: -3.0959 Loss_G: 8.9744\n","[49/70][4/4] Loss_D: -1.0813 Loss_G: -5.8627\n","Epoch 49\n","[50/70][1/4] Loss_D: 2.2847 Loss_G: -17.8508\n","[50/70][2/4] Loss_D: -1.9819 Loss_G: -5.0760\n","[50/70][3/4] Loss_D: 0.0488 Loss_G: -9.6445\n","[50/70][4/4] Loss_D: 0.0517 Loss_G: -15.5424\n","Epoch 50\n","[51/70][1/4] Loss_D: -0.4535 Loss_G: -1.2586\n","[51/70][2/4] Loss_D: -0.6817 Loss_G: 1.3105\n","[51/70][3/4] Loss_D: 0.4010 Loss_G: 7.7966\n","[51/70][4/4] Loss_D: 1.5546 Loss_G: 13.6131\n","Epoch 51\n","[52/70][1/4] Loss_D: 2.4457 Loss_G: 24.6627\n","[52/70][2/4] Loss_D: 3.6503 Loss_G: 31.1224\n","[52/70][3/4] Loss_D: -3.8433 Loss_G: 23.2740\n","[52/70][4/4] Loss_D: -5.4891 Loss_G: 19.7870\n","Epoch 52\n","[53/70][1/4] Loss_D: -4.5737 Loss_G: 22.1508\n","[53/70][2/4] Loss_D: -2.3480 Loss_G: 23.5409\n","[53/70][3/4] Loss_D: -0.1721 Loss_G: 21.6925\n","[53/70][4/4] Loss_D: 0.9732 Loss_G: -0.7638\n","Epoch 53\n","[54/70][1/4] Loss_D: -3.2675 Loss_G: 0.0483\n","[54/70][2/4] Loss_D: -5.6914 Loss_G: 7.4667\n","[54/70][3/4] Loss_D: -4.6351 Loss_G: 8.9016\n","[54/70][4/4] Loss_D: -3.7541 Loss_G: 14.8051\n","Epoch 54\n","[55/70][1/4] Loss_D: -3.3217 Loss_G: 19.0916\n","[55/70][2/4] Loss_D: -2.0926 Loss_G: 14.6666\n","[55/70][3/4] Loss_D: -1.5859 Loss_G: -1.1155\n","[55/70][4/4] Loss_D: -2.0049 Loss_G: 3.3472\n","Epoch 55\n","[56/70][1/4] Loss_D: -3.9096 Loss_G: 16.0633\n","[56/70][2/4] Loss_D: -3.1766 Loss_G: 24.3466\n","[56/70][3/4] Loss_D: -1.7964 Loss_G: 27.6573\n","[56/70][4/4] Loss_D: -0.1622 Loss_G: 26.1362\n","Epoch 56\n","[57/70][1/4] Loss_D: -0.6583 Loss_G: 11.6439\n","[57/70][2/4] Loss_D: -2.5493 Loss_G: 19.1716\n","[57/70][3/4] Loss_D: -2.3119 Loss_G: 29.6835\n","[57/70][4/4] Loss_D: -0.9960 Loss_G: 35.1023\n","Epoch 57\n","[58/70][1/4] Loss_D: -0.2335 Loss_G: 43.6292\n","[58/70][2/4] Loss_D: -3.0861 Loss_G: 19.5032\n","[58/70][3/4] Loss_D: -0.2413 Loss_G: 18.2605\n","[58/70][4/4] Loss_D: -0.4071 Loss_G: 27.1913\n","Epoch 58\n","[59/70][1/4] Loss_D: 1.5120 Loss_G: 26.9862\n","[59/70][2/4] Loss_D: -3.2314 Loss_G: -2.3951\n","[59/70][3/4] Loss_D: -3.8219 Loss_G: -8.1089\n","[59/70][4/4] Loss_D: -2.8150 Loss_G: -9.1619\n","Epoch 59\n","[60/70][1/4] Loss_D: -0.1374 Loss_G: -2.8821\n","[60/70][2/4] Loss_D: 0.8338 Loss_G: 17.5161\n","[60/70][3/4] Loss_D: -3.6275 Loss_G: 31.1409\n","[60/70][4/4] Loss_D: -4.1878 Loss_G: 23.1957\n","Epoch 60\n","[61/70][1/4] Loss_D: -3.1355 Loss_G: 22.9425\n","[61/70][2/4] Loss_D: -1.5891 Loss_G: 24.0811\n","[61/70][3/4] Loss_D: 0.2365 Loss_G: 23.0201\n","[61/70][4/4] Loss_D: 0.6509 Loss_G: 8.7669\n","Epoch 61\n","[62/70][1/4] Loss_D: -3.1602 Loss_G: 12.8451\n","[62/70][2/4] Loss_D: -3.0680 Loss_G: 26.7014\n","[62/70][3/4] Loss_D: -2.5074 Loss_G: 34.5372\n","[62/70][4/4] Loss_D: -0.6728 Loss_G: 42.6951\n","Epoch 62\n","[63/70][1/4] Loss_D: -0.9870 Loss_G: 40.5968\n","[63/70][2/4] Loss_D: -3.6689 Loss_G: 21.0803\n","[63/70][3/4] Loss_D: -2.1272 Loss_G: 17.0800\n","[63/70][4/4] Loss_D: -1.4161 Loss_G: 21.6467\n","Epoch 63\n","[64/70][1/4] Loss_D: -1.9847 Loss_G: -5.8089\n","[64/70][2/4] Loss_D: -1.7954 Loss_G: -3.8607\n","[64/70][3/4] Loss_D: -2.4567 Loss_G: 38.2484\n","[64/70][4/4] Loss_D: -1.4710 Loss_G: 32.1534\n","Epoch 64\n","[65/70][1/4] Loss_D: -1.0797 Loss_G: 19.8549\n","[65/70][2/4] Loss_D: -3.4417 Loss_G: 7.9889\n","[65/70][3/4] Loss_D: -3.0021 Loss_G: 6.0060\n","[65/70][4/4] Loss_D: -1.5567 Loss_G: 16.5976\n","Epoch 65\n","[66/70][1/4] Loss_D: -1.2661 Loss_G: 34.1869\n","[66/70][2/4] Loss_D: -0.9672 Loss_G: 26.6133\n","[66/70][3/4] Loss_D: -1.9845 Loss_G: -6.8691\n","[66/70][4/4] Loss_D: -2.3393 Loss_G: 4.8564\n","Epoch 66\n","[67/70][1/4] Loss_D: -1.0534 Loss_G: 10.4479\n","[67/70][2/4] Loss_D: -0.3273 Loss_G: 21.8834\n","[67/70][3/4] Loss_D: -0.8530 Loss_G: 3.3528\n","[67/70][4/4] Loss_D: -2.5542 Loss_G: 9.5551\n","Epoch 67\n","[68/70][1/4] Loss_D: -2.0229 Loss_G: 22.6283\n","[68/70][2/4] Loss_D: -0.6781 Loss_G: 31.8178\n","[68/70][3/4] Loss_D: -2.1032 Loss_G: 40.5195\n","[68/70][4/4] Loss_D: -1.3279 Loss_G: 29.2393\n","Epoch 68\n","[69/70][1/4] Loss_D: -1.4064 Loss_G: 14.2134\n","[69/70][2/4] Loss_D: -0.6514 Loss_G: 21.6356\n","[69/70][3/4] Loss_D: -1.0983 Loss_G: 0.8216\n","[69/70][4/4] Loss_D: -0.3526 Loss_G: 13.8322\n","Epoch 69\n","[70/70][1/4] Loss_D: -1.4783 Loss_G: 4.4506\n","[70/70][2/4] Loss_D: -2.5018 Loss_G: 20.7274\n","[70/70][3/4] Loss_D: -2.1274 Loss_G: 10.8278\n","[70/70][4/4] Loss_D: -1.6731 Loss_G: 29.7633\n","Epoch 70\n","[71/70][1/4] Loss_D: -1.3807 Loss_G: 9.3966\n","[71/70][2/4] Loss_D: -1.6761 Loss_G: 14.5855\n","[71/70][3/4] Loss_D: -3.2797 Loss_G: -6.5081\n","[71/70][4/4] Loss_D: -0.8895 Loss_G: -4.8243\n","Epoch 71\n","[72/70][1/4] Loss_D: -0.1826 Loss_G: 14.0916\n","[72/70][2/4] Loss_D: -3.1029 Loss_G: 20.1252\n","[72/70][3/4] Loss_D: -2.2058 Loss_G: 30.3672\n","[72/70][4/4] Loss_D: -1.0940 Loss_G: 34.8226\n","Epoch 72\n","[73/70][1/4] Loss_D: 0.0953 Loss_G: 7.3914\n","[73/70][2/4] Loss_D: -3.0579 Loss_G: 10.8565\n","[73/70][3/4] Loss_D: -3.5187 Loss_G: 16.4281\n","[73/70][4/4] Loss_D: -1.9379 Loss_G: 29.6714\n","Epoch 73\n","[74/70][1/4] Loss_D: -1.4572 Loss_G: 43.8021\n","[74/70][2/4] Loss_D: -2.6316 Loss_G: 19.7098\n","[74/70][3/4] Loss_D: -2.0281 Loss_G: 32.4882\n","[74/70][4/4] Loss_D: -0.6450 Loss_G: 21.2839\n","Epoch 74\n","[75/70][1/4] Loss_D: 1.7431 Loss_G: 2.2011\n","[75/70][2/4] Loss_D: -2.2487 Loss_G: -14.1050\n","[75/70][3/4] Loss_D: -0.5943 Loss_G: -2.2184\n","[75/70][4/4] Loss_D: -0.0426 Loss_G: 25.5114\n","Epoch 75\n","[76/70][1/4] Loss_D: -3.0942 Loss_G: 34.0816\n","[76/70][2/4] Loss_D: -2.4248 Loss_G: 32.7732\n","[76/70][3/4] Loss_D: -0.9022 Loss_G: 3.4801\n","[76/70][4/4] Loss_D: -1.3880 Loss_G: -15.3381\n","Epoch 76\n","[77/70][1/4] Loss_D: -1.6939 Loss_G: -9.4306\n","[77/70][2/4] Loss_D: -1.0781 Loss_G: 10.3614\n","[77/70][3/4] Loss_D: -0.8196 Loss_G: 34.2305\n","[77/70][4/4] Loss_D: -3.3984 Loss_G: 42.5185\n","Epoch 77\n","[78/70][1/4] Loss_D: -3.3835 Loss_G: 23.7985\n","[78/70][2/4] Loss_D: -2.6384 Loss_G: 28.3862\n","[78/70][3/4] Loss_D: -0.6918 Loss_G: 21.8747\n","[78/70][4/4] Loss_D: -1.5950 Loss_G: -1.5318\n","Epoch 78\n","[79/70][1/4] Loss_D: -1.5803 Loss_G: -11.1070\n","[79/70][2/4] Loss_D: -0.3563 Loss_G: -6.3664\n","[79/70][3/4] Loss_D: 0.4012 Loss_G: -8.3139\n","[79/70][4/4] Loss_D: -3.3178 Loss_G: 10.9149\n","Epoch 79\n","[80/70][1/4] Loss_D: -3.4994 Loss_G: 14.8945\n","[80/70][2/4] Loss_D: -1.0582 Loss_G: 17.6889\n","[80/70][3/4] Loss_D: 0.4579 Loss_G: -2.3677\n","[80/70][4/4] Loss_D: -1.2869 Loss_G: -6.4005\n","Epoch 80\n","[81/70][1/4] Loss_D: -2.2523 Loss_G: 10.8583\n","[81/70][2/4] Loss_D: -1.3330 Loss_G: 22.8814\n","[81/70][3/4] Loss_D: -0.3592 Loss_G: 35.8773\n","[81/70][4/4] Loss_D: -1.4865 Loss_G: 40.2627\n","Epoch 81\n","[82/70][1/4] Loss_D: -2.3700 Loss_G: 18.6248\n","[82/70][2/4] Loss_D: -0.9509 Loss_G: 20.8419\n","[82/70][3/4] Loss_D: -1.1302 Loss_G: 16.5101\n","[82/70][4/4] Loss_D: -2.4297 Loss_G: 0.3846\n","Epoch 82\n","[83/70][1/4] Loss_D: -1.0478 Loss_G: 2.8180\n","[83/70][2/4] Loss_D: -1.2044 Loss_G: 3.5711\n","[83/70][3/4] Loss_D: -0.7863 Loss_G: 17.4405\n","[83/70][4/4] Loss_D: -1.1112 Loss_G: -2.8632\n","Epoch 83\n","[84/70][1/4] Loss_D: -0.9914 Loss_G: 4.7489\n","[84/70][2/4] Loss_D: -1.4627 Loss_G: 20.5219\n","[84/70][3/4] Loss_D: -0.6325 Loss_G: 19.9693\n","[84/70][4/4] Loss_D: -2.2963 Loss_G: 2.2462\n","Epoch 84\n","[85/70][1/4] Loss_D: -1.6225 Loss_G: 22.3157\n","[85/70][2/4] Loss_D: -1.4711 Loss_G: 45.2671\n","[85/70][3/4] Loss_D: -1.7344 Loss_G: 26.6302\n","[85/70][4/4] Loss_D: -1.2702 Loss_G: 16.3265\n","Epoch 85\n","[86/70][1/4] Loss_D: -1.2068 Loss_G: 30.3753\n","[86/70][2/4] Loss_D: -0.7260 Loss_G: -3.1515\n","[86/70][3/4] Loss_D: -0.1470 Loss_G: 4.8996\n","[86/70][4/4] Loss_D: -0.8110 Loss_G: 37.6741\n","Epoch 86\n","[87/70][1/4] Loss_D: 0.2772 Loss_G: 29.7144\n","[87/70][2/4] Loss_D: -1.6365 Loss_G: -0.5382\n","[87/70][3/4] Loss_D: -2.5050 Loss_G: -9.2410\n","[87/70][4/4] Loss_D: -1.5304 Loss_G: -1.8732\n","Epoch 87\n","[88/70][1/4] Loss_D: -0.5589 Loss_G: 22.1358\n","[88/70][2/4] Loss_D: -0.3383 Loss_G: 30.7989\n","[88/70][3/4] Loss_D: -2.6543 Loss_G: 36.3877\n","[88/70][4/4] Loss_D: -3.6330 Loss_G: 26.1984\n","Epoch 88\n","[89/70][1/4] Loss_D: -1.5288 Loss_G: 29.6036\n","[89/70][2/4] Loss_D: -0.8889 Loss_G: 31.1119\n","[89/70][3/4] Loss_D: -1.4966 Loss_G: -3.3794\n","[89/70][4/4] Loss_D: -1.8020 Loss_G: -6.4560\n","Epoch 89\n","[90/70][1/4] Loss_D: -0.7545 Loss_G: -10.3414\n","[90/70][2/4] Loss_D: -0.6414 Loss_G: 0.7347\n","[90/70][3/4] Loss_D: -2.0752 Loss_G: 21.1214\n","[90/70][4/4] Loss_D: -2.3872 Loss_G: 15.8140\n","Epoch 90\n","[91/70][1/4] Loss_D: -1.0931 Loss_G: 21.3936\n","[91/70][2/4] Loss_D: -1.0218 Loss_G: 16.9600\n","[91/70][3/4] Loss_D: -1.6176 Loss_G: 5.1715\n","[91/70][4/4] Loss_D: -1.3383 Loss_G: -0.5893\n","Epoch 91\n","[92/70][1/4] Loss_D: -0.7783 Loss_G: 24.6065\n","[92/70][2/4] Loss_D: -1.2883 Loss_G: 22.1551\n","[92/70][3/4] Loss_D: -1.9696 Loss_G: 11.4477\n","[92/70][4/4] Loss_D: -0.9587 Loss_G: -4.5665\n","Epoch 92\n","[93/70][1/4] Loss_D: -1.4692 Loss_G: -3.1208\n","[93/70][2/4] Loss_D: -0.7440 Loss_G: 29.1656\n","[93/70][3/4] Loss_D: -1.6341 Loss_G: 21.4550\n","[93/70][4/4] Loss_D: -1.2055 Loss_G: 21.5766\n","Epoch 93\n","[94/70][1/4] Loss_D: -0.6314 Loss_G: -6.6773\n","[94/70][2/4] Loss_D: -1.6786 Loss_G: -3.7931\n","[94/70][3/4] Loss_D: -2.3552 Loss_G: 15.1209\n","[94/70][4/4] Loss_D: -1.0529 Loss_G: 24.5807\n","Epoch 94\n","[95/70][1/4] Loss_D: -0.5608 Loss_G: -6.4384\n","[95/70][2/4] Loss_D: -2.8246 Loss_G: 7.0430\n","[95/70][3/4] Loss_D: -1.9598 Loss_G: 12.9643\n","[95/70][4/4] Loss_D: -1.4988 Loss_G: 25.9925\n","Epoch 95\n","[96/70][1/4] Loss_D: -1.6236 Loss_G: 24.9031\n","[96/70][2/4] Loss_D: -1.9455 Loss_G: 10.5008\n","[96/70][3/4] Loss_D: -1.2130 Loss_G: 19.1565\n","[96/70][4/4] Loss_D: 0.8650 Loss_G: 1.1090\n","Epoch 96\n","[97/70][1/4] Loss_D: -1.3309 Loss_G: -20.4502\n","[97/70][2/4] Loss_D: -2.1274 Loss_G: -8.1350\n","[97/70][3/4] Loss_D: -2.3054 Loss_G: 11.3941\n","[97/70][4/4] Loss_D: -1.5735 Loss_G: 29.5573\n","Epoch 97\n","[98/70][1/4] Loss_D: -1.4051 Loss_G: 24.5773\n","[98/70][2/4] Loss_D: -1.2206 Loss_G: -4.4458\n","[98/70][3/4] Loss_D: -1.1271 Loss_G: -8.7722\n","[98/70][4/4] Loss_D: -0.9447 Loss_G: 23.6568\n","Epoch 98\n","[99/70][1/4] Loss_D: -0.2069 Loss_G: 12.4853\n","[99/70][2/4] Loss_D: -1.1027 Loss_G: 8.3064\n","[99/70][3/4] Loss_D: -0.8615 Loss_G: 37.8073\n","[99/70][4/4] Loss_D: -1.0765 Loss_G: 30.0870\n","Epoch 99\n","[100/70][1/4] Loss_D: -2.3675 Loss_G: 14.5857\n","[100/70][2/4] Loss_D: -1.3467 Loss_G: 13.7114\n","[100/70][3/4] Loss_D: -1.5538 Loss_G: 17.0450\n","[100/70][4/4] Loss_D: -2.2589 Loss_G: 38.7757\n"]}]},{"cell_type":"code","source":["# Representar la pérdida en una gráfica\n","plt.plot(loss_G_values, label='Generator Loss')\n","plt.plot(loss_D_values, label='Discriminator Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","# Guardar la gráfica\n","plt.savefig('/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/loss_graph.png')\n","plt.close()"],"metadata":{"id":"E5fXjOBrO4Wp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Guardar el modelo en un archivo .pth\n","gen_path = \"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/models/generator.pth\"\n","dis_path = \"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/models/discriminator.pth\"\n","\n","torch.save(generator.state_dict(), gen_path)\n","torch.save(discriminator.state_dict(), dis_path)\n","\n","# Función para generar una imagen aleatoria y guardarla en un archivo .jpg\n","def generar_imagen_aleatoria(generator_path, output_path):\n","    # Cargar los pesos del generador desde el archivo .pth\n","    generator = Generator()  # Reemplaza \"Generator()\" con la clase o función que define tu generador\n","    generator.load_state_dict(torch.load(generator_path))\n","    generator.eval()\n","\n","    # Generar una imagen aleatoria\n","    with torch.no_grad():\n","        noise = torch.randn(1, 128, 1, 1)  # Ajusta el tamaño del ruido según tu generador\n","        imagen_generada = generator(noise)\n","\n","    # Guardar la imagen generada en un archivo .jpg\n","    vutils.save_image(imagen_generada, output_path, normalize=True)\n"],"metadata":{"id":"aNfVHFEbQHSN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ejemplo de uso:\n","output_path = \"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v2_Images/models/output.jpg\"\n","generar_imagen_aleatoria(gen_path, output_path)"],"metadata":{"id":"QKgFDLfGt1c0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AEwMELE4QI0L"},"execution_count":null,"outputs":[]}]}