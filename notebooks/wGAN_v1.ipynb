{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xYbvevOpr4D9oc39nUgHe1bAS7rrOu_j","timestamp":1687592383100}],"gpuType":"T4","authorship_tag":"ABX9TyP4jG6N+JTHsfUsijyH0lE/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["https://github.com/ozanciga/gans-with-pytorch/tree/master/wgan-gp"],"metadata":{"id":"8iDPDfODsr0Y"}},{"cell_type":"code","source":["#torch cuda\n","import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"o3O6OVP0srQI","executionInfo":{"status":"ok","timestamp":1687681255288,"user_tz":-120,"elapsed":3514,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coCS0m76swXn","executionInfo":{"status":"ok","timestamp":1687681271499,"user_tz":-120,"elapsed":16213,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}},"outputId":"3e9a3e1c-8158-4d96-e749-2176cdb0f82f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import cv2\n","import numpy as np"],"metadata":{"id":"Xu5UYC9IszG0","executionInfo":{"status":"ok","timestamp":1687681271499,"user_tz":-120,"elapsed":5,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":8,"metadata":{"id":"UAU_eyz3q6Bz","executionInfo":{"status":"ok","timestamp":1687681492435,"user_tz":-120,"elapsed":623,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}}},"outputs":[],"source":["from torch import nn\n","\n","# Residual network.\n","# WGAN-GP paper defines a residual block with up & downsampling.\n","# See the official implementation (given in the paper).\n","# I use architectures described in the official implementation,\n","# since I find it hard to deduce the blocks given here from the text alone.\n","class MeanPoolConv(nn.Module):\n","    def __init__(self, n_input, n_output, k_size):\n","        super(MeanPoolConv, self).__init__()\n","        conv1 = nn.Conv2d(n_input, n_output, k_size, stride=1, padding=(k_size-1)//2, bias=True)\n","        self.model = nn.Sequential(conv1)\n","    def forward(self, x):\n","        out = (x[:,:,::2,::2] + x[:,:,1::2,::2] + x[:,:,::2,1::2] + x[:,:,1::2,1::2]) / 4.0\n","        out = self.model(out)\n","        return out\n","\n","class ConvMeanPool(nn.Module):\n","    def __init__(self, n_input, n_output, k_size):\n","        super(ConvMeanPool, self).__init__()\n","        conv1 = nn.Conv2d(n_input, n_output, k_size, stride=1, padding=(k_size-1)//2, bias=True)\n","        self.model = nn.Sequential(conv1)\n","    def forward(self, x):\n","        out = self.model(x)\n","        out = (out[:,:,::2,::2] + out[:,:,1::2,::2] + out[:,:,::2,1::2] + out[:,:,1::2,1::2]) / 4.0\n","        return out\n","\n","class UpsampleConv(nn.Module):\n","    def __init__(self, n_input, n_output, k_size):\n","        super(UpsampleConv, self).__init__()\n","\n","        self.model = nn.Sequential(\n","            nn.PixelShuffle(2),\n","            nn.Conv2d(n_input, n_output, k_size, stride=1, padding=(k_size-1)//2, bias=True)\n","        )\n","    def forward(self, x):\n","        x = x.repeat((1, 4, 1, 1)) # Weird concat of WGAN-GPs upsampling process.\n","        out = self.model(x)\n","        return out\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, n_input, n_output, k_size, resample='up', bn=True, spatial_dim=None):\n","        super(ResidualBlock, self).__init__()\n","\n","        self.resample = resample\n","\n","        if resample == 'up':\n","            self.conv1 = UpsampleConv(n_input, n_output, k_size)\n","            self.conv2 = nn.Conv2d(n_output, n_output, k_size, padding=(k_size-1)//2)\n","            self.conv_shortcut = UpsampleConv(n_input, n_output, k_size)\n","            self.out_dim = n_output\n","        elif resample == 'down':\n","            self.conv1 = nn.Conv2d(n_input, n_input, k_size, padding=(k_size-1)//2)\n","            self.conv2 = ConvMeanPool(n_input, n_output, k_size)\n","            self.conv_shortcut = ConvMeanPool(n_input, n_output, k_size)\n","            self.out_dim = n_output\n","            self.ln_dims = [n_input, spatial_dim, spatial_dim] # Define the dimensions for layer normalization.\n","        else:\n","            self.conv1 = nn.Conv2d(n_input, n_input, k_size, padding=(k_size-1)//2)\n","            self.conv2 = nn.Conv2d(n_input, n_input, k_size, padding=(k_size-1)//2)\n","            self.conv_shortcut = None # Identity\n","            self.out_dim = n_input\n","            self.ln_dims = [n_input, spatial_dim, spatial_dim]\n","\n","        self.model = nn.Sequential(\n","            nn.BatchNorm2d(n_input) if bn else nn.LayerNorm(self.ln_dims),\n","            nn.ReLU(inplace=True),\n","            self.conv1,\n","            nn.BatchNorm2d(self.out_dim) if bn else nn.LayerNorm(self.ln_dims),\n","            nn.ReLU(inplace=True),\n","            self.conv2,\n","        )\n","\n","    def forward(self, x):\n","        if self.conv_shortcut is None:\n","            return x + self.model(x)\n","        else:\n","            return self.conv_shortcut(x) + self.model(x)\n","\n","class DiscBlock1(nn.Module):\n","    def __init__(self, n_output):\n","        super(DiscBlock1, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, n_output, 3, padding=(3-1)//2)\n","        self.conv2 = ConvMeanPool(n_output, n_output, 1)\n","        self.conv_shortcut = MeanPoolConv(3, n_output, 1)\n","\n","        self.model = nn.Sequential(\n","            self.conv1,\n","            nn.ReLU(inplace=True),\n","            self.conv2\n","        )\n","\n","    def forward(self, x):\n","        return self.conv_shortcut(x) + self.model(x)\n","\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","\n","        self.model = nn.Sequential(                     # 128 x 1 x 1\n","            nn.ConvTranspose2d(128, 128, 4, 1, 0),      # 128 x 4 x 4\n","            #nn.ConvTranspose2d(128, 128, 4, 2, 1),     #Para 64x64 pixeles de entrada\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 8 x 8\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 16 x 16\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 32 x 32\n","            ResidualBlock(128, 128, 3, resample='up'),  # 128 x 64 x 64\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(128, 3, 3, padding=(3-1)//2),     # 3 x 64 x 64\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, z):\n","        img = self.model(z)\n","        return img\n","\n","\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        n_output = 128\n","        '''\n","        This is a parameter but since we experiment with a single size\n","        of 3 x 32 x 32 images, it is hardcoded here.\n","        '''\n","\n","        self.DiscBlock1 = DiscBlock1(n_output)                      # 128 x 32 x 32\n","\n","        self.model = nn.Sequential(\n","            ResidualBlock(n_output, n_output, 3, resample='down', bn=False, spatial_dim=32),  # 128 x 16 x 16\n","            ResidualBlock(n_output, n_output, 3, resample='down', bn=False, spatial_dim=16),  # 128 x 8 x 8\n","            ResidualBlock(n_output, n_output, 3, resample=None, bn=False, spatial_dim=8),    # 128 x 8 x 8\n","            ResidualBlock(n_output, n_output, 3, resample=None, bn=False, spatial_dim=8),    # 128 x 8 x 8\n","            nn.ReLU(inplace=True),\n","        )\n","        self.l1 = nn.Sequential(nn.Linear(128, 1))                  # 128 x 1\n","\n","    def forward(self, x):\n","        # x = x.view(-1, 3, 32, 32)\n","        y = self.DiscBlock1(x)\n","        y = self.model(y)\n","        y = y.view(x.size(0), 128, -1)\n","        y = y.mean(dim=2)\n","        out = self.l1(y).unsqueeze_(1).unsqueeze_(2) # or *.view(x.size(0), 128, 1, 1, 1)\n","        return out\n","\n","\n"]},{"cell_type":"code","source":["import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","class CustomDataset(data.Dataset):\n","    def __init__(self, X_folder, y_folder, transform=None):\n","        self.X_folder = X_folder\n","        self.y_folder = y_folder\n","        self.transform = transform\n","\n","        # Obtener la lista de nombres de archivo en las carpetas\n","        self.X_filenames = [filename for filename in os.listdir(X_folder) if filename.endswith('.jpg')]\n","        self.y_filenames = [filename for filename in os.listdir(y_folder) if filename.endswith('.jpg')]\n","\n","        #self.resizer = transforms.Resize((64, 64))  # Redimensionar las imágenes a 64x64 píxeles\n","        #self.to_grayscale = transforms.Grayscale()  # Convertir las imágenes a escala de grises\n","\n","    def __len__(self):\n","        return len(self.X_filenames)\n","\n","    def __getitem__(self, index):\n","        X_filename = self.X_filenames[index]\n","        y_filename = self.y_filenames[index]\n","\n","        if not X_filename.endswith(\".jpg\"):\n","            return self.__getitem__((index + 1) % len(self))\n","\n","        # Cargar las imágenes y las etiquetas\n","        X = Image.open(os.path.join(self.X_folder, X_filename))\n","        y = Image.open(os.path.join(self.y_folder, y_filename))\n","\n","        # Redimensionar las imágenes a 64x64 píxeles y convertirlas a escala de grises\n","        #X = self.resizer(X)\n","        #X = self.to_grayscale(X)\n","        #y = self.resizer(y)\n","        #y = self.to_grayscale(y)\n","\n","        if self.transform:\n","            X = self.transform(X)\n","            y = self.transform(y)\n","\n","        return X, y\n","\n","    def get_images(self):\n","        images = []\n","        for X_filename in self.X_filenames:\n","            X = Image.open(os.path.join(self.X_folder, X_filename))#.convert(\"RGB\")\n","            #X = self.resizer(X)\n","            if self.transform:\n","                X = self.transform(X)\n","            images.append(X)\n","        return images\n","\n","    def get_labels(self):\n","        labels = []\n","        for y_filename in self.y_filenames:\n","            y = Image.open(os.path.join(self.y_folder, y_filename))#.convert(\"RGB\")\n","            #y = self.resizer(y)\n","            if self.transform:\n","                y = self.transform(y)\n","            labels.append(y)\n","        return labels\n"],"metadata":{"id":"Wf9iRqQjuEtZ","executionInfo":{"status":"ok","timestamp":1687681277333,"user_tz":-120,"elapsed":1147,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn, optim\n","from torch.autograd.variable import Variable\n","\n","from torchvision import transforms, datasets\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n","\n","import torchvision.utils as vutils\n","\n","import errno\n","\n","class Args:\n","    def __init__(self):\n","        self.n_epochs = 100\n","        self.batch_size = 64\n","        self.alpha = 0.0001\n","        self.b1 = 0.5\n","        self.b2 = 0.9\n","        self.n_critic = 5\n","        self.lambda_1 = 10\n","        self.img_size = 64\n","        self.channels = 3\n","        #self.display_port = 8097\n","        #self.display_server = \"http://localhost\"\n","        self.sample_interval = 256\n","opt = Args()\n","\n","\n","img_dims = (opt.channels, opt.img_size, opt.img_size)\n","n_features = opt.channels * opt.img_size * opt.img_size\n","\n","# TODO: Use some initialization in the future.\n","def init_weights(m):\n","    if type(m) == nn.ConvTranspose2d:\n","        torch.nn.init.kaiming_normal(m.weight, mode='fan_out', nonlinearity='relu')\n","    elif type(m) == nn.Conv2d:\n","        torch.nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n","\n","# Definir las rutas de las carpetas de entrenamiento y prueba\n","trainX_folder = '/content/gdrive/MyDrive/Dev/AI_MsC/TFM/CRACK500/traindata/traindata/'\n","trainy_folder = '/content/gdrive/MyDrive/Dev/AI_MsC/TFM/CRACK500/valdata/valdata/'\n","\n","# Crear las transformaciones\n","transform = transforms.Compose([\n","    transforms.Resize((opt.img_size, opt.img_size)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n","])\n","\n","# Crear el dataset personalizado de entrenamiento\n","dataset_full = CustomDataset(trainX_folder, trainy_folder, transform=transform)\n","# Obtener xtrain e ytrain\n","dataset = dataset_full.get_images()\n","# Crear los dataloaders\n","batch_iterator = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=2)\n","\n","cuda = torch.cuda.is_available()\n","Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","gan_loss = nn.BCELoss()\n","\n","generator = Generator()\n","discriminator = Discriminator()\n","\n","optimizer_D = optim.Adam(discriminator.parameters(), lr=opt.alpha, betas=(opt.b1, opt.b2))\n","optimizer_G = optim.Adam(generator.parameters(), lr=opt.alpha, betas=(opt.b1, opt.b2))\n","\n","# Loss record.\n","g_losses = []\n","d_losses = []\n","epochs = []\n","loss_legend = ['Discriminator', 'Generator']\n","\n","if cuda:\n","    generator = generator.cuda()\n","    discriminator = discriminator.cuda()\n","\n","noise_fixed = Variable(Tensor(25, 128, 1, 1).normal_(0, 1), requires_grad=False) # To track the progress of the GAN.\n","\n","for epoch in range(opt.n_epochs):\n","    print('Epoch {}'.format(epoch))\n","    for i, batch in enumerate(batch_iterator):\n","        # == Discriminator update == #\n","        for iter in range(opt.n_critic):\n","            # Sample real and fake images, using notation in paper.\n","            x = Variable(batch.type(Tensor))\n","            noise = Variable(Tensor(batch.size(0), 128, 1, 1).normal_(0, 1))\n","            x_tilde = Variable(generator(noise), requires_grad=True)\n","\n","            epsilon = Variable(Tensor(batch.size(0), 1, 1, 1).uniform_(0, 1))\n","\n","            x_hat = epsilon*x + (1 - epsilon)*x_tilde\n","            x_hat = torch.autograd.Variable(x_hat, requires_grad=True)\n","\n","            # Put the interpolated data through critic.\n","            dw_x = discriminator(x_hat)\n","            # A great exercise on learning how the autograd.grad works!\n","            grad_x = torch.autograd.grad(outputs=dw_x, inputs=x_hat,\n","                                         grad_outputs=Variable(Tensor(batch.size(0), 1, 1, 1).fill_(1.0), requires_grad=False),\n","                                         create_graph=True, retain_graph=True, only_inputs=True)\n","            grad_x = grad_x[0].view(batch.size(0), -1)\n","            grad_x = grad_x.norm(p=2, dim=1) # My naming is inaccurate, this is the 2-norm of grad(D_w(x_hat))\n","\n","            # Update discriminator (or critic, since we don't output probabilities anymore).\n","            optimizer_D.zero_grad()\n","\n","            # WGAN-GP loss, defined properly as a loss unlike the WGAN paper.\n","            d_loss = torch.mean(discriminator(x_tilde)) - torch.mean(discriminator(x)) + opt.lambda_1*torch.mean((grad_x - 1)**2)\n","            # d_loss = torch.mean(d_loss) # there's a reason for why this shouldn't be done this way :)\n","\n","            d_loss.backward()\n","            optimizer_D.step()\n","\n","        # == Generator update == #\n","        noise = Variable(Tensor(batch.size(0), 128, 1, 1).normal_(0, 1))\n","        imgs_fake = generator(noise)\n","\n","        optimizer_G.zero_grad()\n","\n","        g_loss = -torch.mean(discriminator(imgs_fake))\n","\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch+1, 70, i+1, len(batch_iterator), d_loss.data, g_loss.data))\n","        if i % 100 == 0: # Every 100 steps:\n","            vutils.save_image(x, '/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v1_Images/real_samples.png', normalize = True) # We save the real images of the minibatch.\n","            fake = generator(noise) # We get our fake generated images.\n","            vutils.save_image(fake.detach(), f\"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v1_Images/fake_samples/fake_samples_epoch_{epoch:03d}.png\", normalize=True) # We also save the fake generated images of the minibatch.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdvQ52nirB2X","executionInfo":{"status":"ok","timestamp":1687682840057,"user_tz":-120,"elapsed":1344817,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}},"outputId":"d0d8bc00-2b31-4e92-a0a8-434ea89ac006"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0\n","[1/70][1/4] Loss_D: 4.4981 Loss_G: 2.2033\n","[1/70][2/4] Loss_D: -3.5825 Loss_G: 3.3254\n","[1/70][3/4] Loss_D: -9.8524 Loss_G: 5.0289\n","[1/70][4/4] Loss_D: -14.6230 Loss_G: 5.2666\n","Epoch 1\n","[2/70][1/4] Loss_D: -17.9885 Loss_G: 6.8849\n","[2/70][2/4] Loss_D: -23.9062 Loss_G: 6.2201\n","[2/70][3/4] Loss_D: -24.5498 Loss_G: 6.2076\n","[2/70][4/4] Loss_D: -23.5531 Loss_G: 6.2552\n","Epoch 2\n","[3/70][1/4] Loss_D: -21.3566 Loss_G: -4.0157\n","[3/70][2/4] Loss_D: -19.8737 Loss_G: 5.2533\n","[3/70][3/4] Loss_D: -23.0726 Loss_G: 6.9850\n","[3/70][4/4] Loss_D: -16.1973 Loss_G: -1.9666\n","Epoch 3\n","[4/70][1/4] Loss_D: -18.0114 Loss_G: -11.6285\n","[4/70][2/4] Loss_D: -17.7231 Loss_G: -10.7598\n","[4/70][3/4] Loss_D: -14.1109 Loss_G: 12.8975\n","[4/70][4/4] Loss_D: -16.9094 Loss_G: -5.1044\n","Epoch 4\n","[5/70][1/4] Loss_D: -14.3335 Loss_G: -11.5591\n","[5/70][2/4] Loss_D: -10.7606 Loss_G: -2.3974\n","[5/70][3/4] Loss_D: -5.4305 Loss_G: -11.2291\n","[5/70][4/4] Loss_D: -6.3470 Loss_G: -21.3477\n","Epoch 5\n","[6/70][1/4] Loss_D: -10.9703 Loss_G: -13.2586\n","[6/70][2/4] Loss_D: -12.5154 Loss_G: -16.8014\n","[6/70][3/4] Loss_D: -7.8037 Loss_G: -22.9872\n","[6/70][4/4] Loss_D: -6.3579 Loss_G: -6.0347\n","Epoch 6\n","[7/70][1/4] Loss_D: -8.6500 Loss_G: -6.5918\n","[7/70][2/4] Loss_D: -7.4209 Loss_G: -9.5937\n","[7/70][3/4] Loss_D: -6.0282 Loss_G: -11.6934\n","[7/70][4/4] Loss_D: -4.7629 Loss_G: -19.9342\n","Epoch 7\n","[8/70][1/4] Loss_D: -4.2259 Loss_G: -14.9028\n","[8/70][2/4] Loss_D: -3.9801 Loss_G: -16.7843\n","[8/70][3/4] Loss_D: -4.1908 Loss_G: -12.9882\n","[8/70][4/4] Loss_D: -5.8327 Loss_G: 2.6498\n","Epoch 8\n","[9/70][1/4] Loss_D: -5.1227 Loss_G: -5.4328\n","[9/70][2/4] Loss_D: -5.4418 Loss_G: -0.8695\n","[9/70][3/4] Loss_D: -3.5955 Loss_G: -16.1457\n","[9/70][4/4] Loss_D: -4.5292 Loss_G: 16.9853\n","Epoch 9\n","[10/70][1/4] Loss_D: -6.0188 Loss_G: -11.3882\n","[10/70][2/4] Loss_D: -5.8238 Loss_G: -6.1956\n","[10/70][3/4] Loss_D: -7.2512 Loss_G: -6.8084\n","[10/70][4/4] Loss_D: -6.8798 Loss_G: 4.8404\n","Epoch 10\n","[11/70][1/4] Loss_D: -6.7993 Loss_G: -4.1519\n","[11/70][2/4] Loss_D: -5.9521 Loss_G: -2.7526\n","[11/70][3/4] Loss_D: -6.3186 Loss_G: -4.7597\n","[11/70][4/4] Loss_D: -6.6822 Loss_G: 2.4658\n","Epoch 11\n","[12/70][1/4] Loss_D: -6.0933 Loss_G: -0.6500\n","[12/70][2/4] Loss_D: -5.8014 Loss_G: 3.4341\n","[12/70][3/4] Loss_D: -4.9652 Loss_G: 1.7167\n","[12/70][4/4] Loss_D: -6.1911 Loss_G: 3.1576\n","Epoch 12\n","[13/70][1/4] Loss_D: -4.3739 Loss_G: 0.2258\n","[13/70][2/4] Loss_D: -5.7776 Loss_G: -1.4471\n","[13/70][3/4] Loss_D: -3.0749 Loss_G: -14.9218\n","[13/70][4/4] Loss_D: -1.9456 Loss_G: -1.6209\n","Epoch 13\n","[14/70][1/4] Loss_D: -3.7944 Loss_G: 0.1362\n","[14/70][2/4] Loss_D: -5.4775 Loss_G: -7.0103\n","[14/70][3/4] Loss_D: -4.3662 Loss_G: -2.6196\n","[14/70][4/4] Loss_D: -3.6201 Loss_G: -23.3123\n","Epoch 14\n","[15/70][1/4] Loss_D: -3.2487 Loss_G: -21.9240\n","[15/70][2/4] Loss_D: -1.2725 Loss_G: -16.2338\n","[15/70][3/4] Loss_D: -3.4355 Loss_G: 0.3531\n","[15/70][4/4] Loss_D: -4.7094 Loss_G: -10.3190\n","Epoch 15\n","[16/70][1/4] Loss_D: -2.0944 Loss_G: -28.5033\n","[16/70][2/4] Loss_D: -2.8479 Loss_G: -28.7658\n","[16/70][3/4] Loss_D: -0.7365 Loss_G: -37.0171\n","[16/70][4/4] Loss_D: -1.3139 Loss_G: -31.1266\n","Epoch 16\n","[17/70][1/4] Loss_D: -4.6938 Loss_G: -15.2731\n","[17/70][2/4] Loss_D: -2.8028 Loss_G: -13.3748\n","[17/70][3/4] Loss_D: 0.5267 Loss_G: -19.5287\n","[17/70][4/4] Loss_D: -1.9809 Loss_G: -16.5836\n","Epoch 17\n","[18/70][1/4] Loss_D: -7.3735 Loss_G: -22.0838\n","[18/70][2/4] Loss_D: -4.4732 Loss_G: -20.8639\n","[18/70][3/4] Loss_D: -0.5776 Loss_G: -23.5496\n","[18/70][4/4] Loss_D: -1.6933 Loss_G: -15.5820\n","Epoch 18\n","[19/70][1/4] Loss_D: -5.9663 Loss_G: -5.2769\n","[19/70][2/4] Loss_D: -4.4475 Loss_G: -5.4769\n","[19/70][3/4] Loss_D: -2.5409 Loss_G: -8.6110\n","[19/70][4/4] Loss_D: -1.5322 Loss_G: -0.8612\n","Epoch 19\n","[20/70][1/4] Loss_D: -2.7856 Loss_G: 0.6009\n","[20/70][2/4] Loss_D: -3.8950 Loss_G: -1.9041\n","[20/70][3/4] Loss_D: -3.2015 Loss_G: -1.8249\n","[20/70][4/4] Loss_D: -0.8182 Loss_G: 6.1659\n","Epoch 20\n","[21/70][1/4] Loss_D: -4.4727 Loss_G: -0.1815\n","[21/70][2/4] Loss_D: -4.3427 Loss_G: 1.3058\n","[21/70][3/4] Loss_D: -2.6675 Loss_G: 2.4806\n","[21/70][4/4] Loss_D: -1.2037 Loss_G: -18.0968\n","Epoch 21\n","[22/70][1/4] Loss_D: -5.1623 Loss_G: 3.5653\n","[22/70][2/4] Loss_D: -3.7662 Loss_G: 15.7993\n","[22/70][3/4] Loss_D: -3.5351 Loss_G: 12.3909\n","[22/70][4/4] Loss_D: -4.2383 Loss_G: -4.6930\n","Epoch 22\n","[23/70][1/4] Loss_D: -3.8725 Loss_G: 5.2769\n","[23/70][2/4] Loss_D: -3.1469 Loss_G: 4.5840\n","[23/70][3/4] Loss_D: -2.6620 Loss_G: -15.6006\n","[23/70][4/4] Loss_D: -4.1967 Loss_G: -2.8333\n","Epoch 23\n","[24/70][1/4] Loss_D: -2.8079 Loss_G: -15.3887\n","[24/70][2/4] Loss_D: -3.8322 Loss_G: -17.3790\n","[24/70][3/4] Loss_D: -5.4903 Loss_G: -17.2653\n","[24/70][4/4] Loss_D: -4.8939 Loss_G: -3.6463\n","Epoch 24\n","[25/70][1/4] Loss_D: -1.0824 Loss_G: -5.4876\n","[25/70][2/4] Loss_D: -3.1131 Loss_G: -0.0296\n","[25/70][3/4] Loss_D: -3.7775 Loss_G: -3.5140\n","[25/70][4/4] Loss_D: -3.1990 Loss_G: -0.3213\n","Epoch 25\n","[26/70][1/4] Loss_D: -2.8883 Loss_G: 5.2397\n","[26/70][2/4] Loss_D: -2.7926 Loss_G: -6.7531\n","[26/70][3/4] Loss_D: -3.8654 Loss_G: -9.3862\n","[26/70][4/4] Loss_D: -2.4397 Loss_G: -12.8091\n","Epoch 26\n","[27/70][1/4] Loss_D: -5.3450 Loss_G: -4.0281\n","[27/70][2/4] Loss_D: -5.1523 Loss_G: -6.9614\n","[27/70][3/4] Loss_D: -4.7869 Loss_G: -2.3091\n","[27/70][4/4] Loss_D: -4.7097 Loss_G: -8.2250\n","Epoch 27\n","[28/70][1/4] Loss_D: -4.0278 Loss_G: -0.0608\n","[28/70][2/4] Loss_D: -4.0197 Loss_G: 3.6939\n","[28/70][3/4] Loss_D: -4.3287 Loss_G: 9.7876\n","[28/70][4/4] Loss_D: -3.9785 Loss_G: 7.3747\n","Epoch 28\n","[29/70][1/4] Loss_D: -3.7350 Loss_G: 2.3262\n","[29/70][2/4] Loss_D: -4.6548 Loss_G: 1.7549\n","[29/70][3/4] Loss_D: -3.3373 Loss_G: -1.2846\n","[29/70][4/4] Loss_D: -4.6640 Loss_G: -0.4189\n","Epoch 29\n","[30/70][1/4] Loss_D: -3.8961 Loss_G: 0.0591\n","[30/70][2/4] Loss_D: -4.1543 Loss_G: -12.1033\n","[30/70][3/4] Loss_D: -3.2313 Loss_G: -11.0883\n","[30/70][4/4] Loss_D: -2.9388 Loss_G: -14.2403\n","Epoch 30\n","[31/70][1/4] Loss_D: -4.1033 Loss_G: -5.0196\n","[31/70][2/4] Loss_D: -2.6521 Loss_G: -15.9117\n","[31/70][3/4] Loss_D: -3.6720 Loss_G: -11.6844\n","[31/70][4/4] Loss_D: -4.6394 Loss_G: 7.3283\n","Epoch 31\n","[32/70][1/4] Loss_D: -4.2379 Loss_G: -0.8942\n","[32/70][2/4] Loss_D: -2.6764 Loss_G: -4.3012\n","[32/70][3/4] Loss_D: -4.4986 Loss_G: -2.5394\n","[32/70][4/4] Loss_D: -4.2123 Loss_G: -12.4678\n","Epoch 32\n","[33/70][1/4] Loss_D: -4.3100 Loss_G: -11.3426\n","[33/70][2/4] Loss_D: -3.8012 Loss_G: -13.1762\n","[33/70][3/4] Loss_D: -2.8123 Loss_G: -3.3988\n","[33/70][4/4] Loss_D: -4.0887 Loss_G: -8.7068\n","Epoch 33\n","[34/70][1/4] Loss_D: -3.2122 Loss_G: 11.3856\n","[34/70][2/4] Loss_D: -4.7767 Loss_G: -1.8537\n","[34/70][3/4] Loss_D: -5.2247 Loss_G: -3.6218\n","[34/70][4/4] Loss_D: -3.3087 Loss_G: -9.9338\n","Epoch 34\n","[35/70][1/4] Loss_D: -4.4232 Loss_G: -5.7374\n","[35/70][2/4] Loss_D: -1.4574 Loss_G: -13.1846\n","[35/70][3/4] Loss_D: -3.8494 Loss_G: -18.6581\n","[35/70][4/4] Loss_D: -3.2023 Loss_G: -5.4315\n","Epoch 35\n","[36/70][1/4] Loss_D: -5.3391 Loss_G: -0.6040\n","[36/70][2/4] Loss_D: -1.9393 Loss_G: -12.1804\n","[36/70][3/4] Loss_D: -3.7533 Loss_G: -10.9214\n","[36/70][4/4] Loss_D: -4.6913 Loss_G: -6.6527\n","Epoch 36\n","[37/70][1/4] Loss_D: -3.7779 Loss_G: 1.9180\n","[37/70][2/4] Loss_D: -3.9211 Loss_G: -5.6223\n","[37/70][3/4] Loss_D: -3.8066 Loss_G: -5.4270\n","[37/70][4/4] Loss_D: -2.6969 Loss_G: 0.8677\n","Epoch 37\n","[38/70][1/4] Loss_D: -4.0209 Loss_G: 1.3602\n","[38/70][2/4] Loss_D: -2.9519 Loss_G: 2.9640\n","[38/70][3/4] Loss_D: -1.2532 Loss_G: 2.7109\n","[38/70][4/4] Loss_D: -2.8326 Loss_G: 2.2677\n","Epoch 38\n","[39/70][1/4] Loss_D: -2.6457 Loss_G: 1.4031\n","[39/70][2/4] Loss_D: -1.5044 Loss_G: -5.0761\n","[39/70][3/4] Loss_D: -3.8168 Loss_G: 5.0309\n","[39/70][4/4] Loss_D: -2.9431 Loss_G: 8.8588\n","Epoch 39\n","[40/70][1/4] Loss_D: -3.6366 Loss_G: -14.0649\n","[40/70][2/4] Loss_D: -5.5315 Loss_G: -12.8712\n","[40/70][3/4] Loss_D: -3.0026 Loss_G: -0.7741\n","[40/70][4/4] Loss_D: -3.0829 Loss_G: -7.1687\n","Epoch 40\n","[41/70][1/4] Loss_D: -3.3053 Loss_G: -1.5912\n","[41/70][2/4] Loss_D: -3.9933 Loss_G: -6.4280\n","[41/70][3/4] Loss_D: -2.5813 Loss_G: -1.5294\n","[41/70][4/4] Loss_D: -3.7483 Loss_G: -4.8847\n","Epoch 41\n","[42/70][1/4] Loss_D: -2.6681 Loss_G: -4.7171\n","[42/70][2/4] Loss_D: -4.2175 Loss_G: -7.4927\n","[42/70][3/4] Loss_D: -1.8600 Loss_G: 2.1354\n","[42/70][4/4] Loss_D: -3.4747 Loss_G: -2.7405\n","Epoch 42\n","[43/70][1/4] Loss_D: -2.7535 Loss_G: -6.2919\n","[43/70][2/4] Loss_D: -3.8179 Loss_G: -10.7938\n","[43/70][3/4] Loss_D: -2.8129 Loss_G: -9.5086\n","[43/70][4/4] Loss_D: -3.3555 Loss_G: -3.6475\n","Epoch 43\n","[44/70][1/4] Loss_D: -4.5702 Loss_G: -0.7963\n","[44/70][2/4] Loss_D: -2.7127 Loss_G: -2.1514\n","[44/70][3/4] Loss_D: -1.9875 Loss_G: -11.8220\n","[44/70][4/4] Loss_D: -3.2972 Loss_G: -17.8447\n","Epoch 44\n","[45/70][1/4] Loss_D: -3.8574 Loss_G: -12.1870\n","[45/70][2/4] Loss_D: -1.7797 Loss_G: -1.0902\n","[45/70][3/4] Loss_D: -3.2514 Loss_G: 2.4244\n","[45/70][4/4] Loss_D: -3.0374 Loss_G: 2.8514\n","Epoch 45\n","[46/70][1/4] Loss_D: -3.1118 Loss_G: 8.4861\n","[46/70][2/4] Loss_D: -3.0036 Loss_G: 12.2829\n","[46/70][3/4] Loss_D: -3.8764 Loss_G: 14.7233\n","[46/70][4/4] Loss_D: -3.2854 Loss_G: 8.4626\n","Epoch 46\n","[47/70][1/4] Loss_D: -3.7565 Loss_G: -1.4914\n","[47/70][2/4] Loss_D: -3.2091 Loss_G: -6.0272\n","[47/70][3/4] Loss_D: -3.5459 Loss_G: -7.8910\n","[47/70][4/4] Loss_D: -3.1659 Loss_G: -6.9599\n","Epoch 47\n","[48/70][1/4] Loss_D: -4.1345 Loss_G: 6.0538\n","[48/70][2/4] Loss_D: -3.5158 Loss_G: 2.5239\n","[48/70][3/4] Loss_D: -3.7874 Loss_G: -2.2804\n","[48/70][4/4] Loss_D: -4.3206 Loss_G: 3.7632\n","Epoch 48\n","[49/70][1/4] Loss_D: -4.1467 Loss_G: -1.4163\n","[49/70][2/4] Loss_D: -4.4072 Loss_G: -3.3570\n","[49/70][3/4] Loss_D: -3.0176 Loss_G: -5.5025\n","[49/70][4/4] Loss_D: -2.5268 Loss_G: -20.1298\n","Epoch 49\n","[50/70][1/4] Loss_D: -2.9372 Loss_G: -22.9131\n","[50/70][2/4] Loss_D: -2.9474 Loss_G: 4.1307\n","[50/70][3/4] Loss_D: -1.5043 Loss_G: -10.2038\n","[50/70][4/4] Loss_D: -3.4490 Loss_G: 0.6737\n","Epoch 50\n","[51/70][1/4] Loss_D: -3.9331 Loss_G: 9.0363\n","[51/70][2/4] Loss_D: -2.7840 Loss_G: -8.3452\n","[51/70][3/4] Loss_D: -3.9618 Loss_G: -5.7988\n","[51/70][4/4] Loss_D: -4.0295 Loss_G: -6.5885\n","Epoch 51\n","[52/70][1/4] Loss_D: -4.2764 Loss_G: 1.2504\n","[52/70][2/4] Loss_D: -4.1384 Loss_G: -2.6311\n","[52/70][3/4] Loss_D: -3.4304 Loss_G: 4.6531\n","[52/70][4/4] Loss_D: -4.5320 Loss_G: -2.5898\n","Epoch 52\n","[53/70][1/4] Loss_D: -4.1609 Loss_G: 0.2305\n","[53/70][2/4] Loss_D: -4.0757 Loss_G: 0.2118\n","[53/70][3/4] Loss_D: -4.3181 Loss_G: -10.1960\n","[53/70][4/4] Loss_D: -3.6992 Loss_G: -19.9121\n","Epoch 53\n","[54/70][1/4] Loss_D: -4.3649 Loss_G: 2.3480\n","[54/70][2/4] Loss_D: -3.3320 Loss_G: 1.6545\n","[54/70][3/4] Loss_D: -4.0182 Loss_G: -4.2153\n","[54/70][4/4] Loss_D: -3.5955 Loss_G: -2.4281\n","Epoch 54\n","[55/70][1/4] Loss_D: -4.2017 Loss_G: 10.3107\n","[55/70][2/4] Loss_D: -4.3253 Loss_G: 4.0346\n","[55/70][3/4] Loss_D: -1.9291 Loss_G: 6.3440\n","[55/70][4/4] Loss_D: -2.7392 Loss_G: -4.7486\n","Epoch 55\n","[56/70][1/4] Loss_D: -3.6439 Loss_G: -7.2442\n","[56/70][2/4] Loss_D: -3.6062 Loss_G: 4.1590\n","[56/70][3/4] Loss_D: -3.7241 Loss_G: 4.2614\n","[56/70][4/4] Loss_D: -5.4182 Loss_G: -1.8164\n","Epoch 56\n","[57/70][1/4] Loss_D: -2.4792 Loss_G: 4.1725\n","[57/70][2/4] Loss_D: -4.2684 Loss_G: 4.2945\n","[57/70][3/4] Loss_D: -4.2118 Loss_G: 1.0924\n","[57/70][4/4] Loss_D: -2.6894 Loss_G: -2.3034\n","Epoch 57\n","[58/70][1/4] Loss_D: -3.3594 Loss_G: 2.1819\n","[58/70][2/4] Loss_D: -3.9255 Loss_G: -8.3715\n","[58/70][3/4] Loss_D: -3.6312 Loss_G: 9.1924\n","[58/70][4/4] Loss_D: -3.6342 Loss_G: 4.3531\n","Epoch 58\n","[59/70][1/4] Loss_D: -3.2197 Loss_G: 0.6396\n","[59/70][2/4] Loss_D: -4.2422 Loss_G: -4.0479\n","[59/70][3/4] Loss_D: -4.4657 Loss_G: -0.6534\n","[59/70][4/4] Loss_D: -3.7339 Loss_G: 0.1290\n","Epoch 59\n","[60/70][1/4] Loss_D: -4.9124 Loss_G: -4.7947\n","[60/70][2/4] Loss_D: -3.1030 Loss_G: 1.6157\n","[60/70][3/4] Loss_D: -3.9154 Loss_G: 2.8253\n","[60/70][4/4] Loss_D: -4.2335 Loss_G: 3.6907\n","Epoch 60\n","[61/70][1/4] Loss_D: -3.5209 Loss_G: 3.5569\n","[61/70][2/4] Loss_D: -2.9953 Loss_G: 11.0423\n","[61/70][3/4] Loss_D: -4.5500 Loss_G: 6.7741\n","[61/70][4/4] Loss_D: -4.4886 Loss_G: -7.0405\n","Epoch 61\n","[62/70][1/4] Loss_D: -2.7735 Loss_G: -7.9426\n","[62/70][2/4] Loss_D: -4.7701 Loss_G: 5.1069\n","[62/70][3/4] Loss_D: -3.3089 Loss_G: 10.1128\n","[62/70][4/4] Loss_D: -3.6386 Loss_G: 0.5237\n","Epoch 62\n","[63/70][1/4] Loss_D: -4.5944 Loss_G: -2.5630\n","[63/70][2/4] Loss_D: -3.0013 Loss_G: 20.1228\n","[63/70][3/4] Loss_D: -1.1243 Loss_G: -23.1651\n","[63/70][4/4] Loss_D: -3.6266 Loss_G: 17.2397\n","Epoch 63\n","[64/70][1/4] Loss_D: -2.5471 Loss_G: 8.3336\n","[64/70][2/4] Loss_D: -4.5153 Loss_G: -1.6373\n","[64/70][3/4] Loss_D: -3.8857 Loss_G: 10.4901\n","[64/70][4/4] Loss_D: -2.7595 Loss_G: 8.4199\n","Epoch 64\n","[65/70][1/4] Loss_D: -2.3160 Loss_G: 4.0474\n","[65/70][2/4] Loss_D: -3.3012 Loss_G: -12.0587\n","[65/70][3/4] Loss_D: -3.7031 Loss_G: -6.3018\n","[65/70][4/4] Loss_D: -5.7541 Loss_G: 7.9625\n","Epoch 65\n","[66/70][1/4] Loss_D: -4.3911 Loss_G: 7.1187\n","[66/70][2/4] Loss_D: -4.9458 Loss_G: 1.5438\n","[66/70][3/4] Loss_D: -3.7601 Loss_G: 2.0496\n","[66/70][4/4] Loss_D: -4.7281 Loss_G: 2.3104\n","Epoch 66\n","[67/70][1/4] Loss_D: -4.7273 Loss_G: 11.2282\n","[67/70][2/4] Loss_D: -3.3684 Loss_G: 1.3854\n","[67/70][3/4] Loss_D: -3.4987 Loss_G: 10.3317\n","[67/70][4/4] Loss_D: -3.8258 Loss_G: 18.1942\n","Epoch 67\n","[68/70][1/4] Loss_D: -4.0996 Loss_G: 11.8481\n","[68/70][2/4] Loss_D: -3.3626 Loss_G: 8.8920\n","[68/70][3/4] Loss_D: -3.7477 Loss_G: 6.6847\n","[68/70][4/4] Loss_D: -3.3057 Loss_G: 0.6352\n","Epoch 68\n","[69/70][1/4] Loss_D: -3.8915 Loss_G: 5.7766\n","[69/70][2/4] Loss_D: -3.2010 Loss_G: 13.2302\n","[69/70][3/4] Loss_D: -2.9239 Loss_G: 14.2608\n","[69/70][4/4] Loss_D: -2.9967 Loss_G: 15.3475\n","Epoch 69\n","[70/70][1/4] Loss_D: -3.7313 Loss_G: 9.1070\n","[70/70][2/4] Loss_D: -3.6071 Loss_G: 17.6176\n","[70/70][3/4] Loss_D: -3.4514 Loss_G: 10.7436\n","[70/70][4/4] Loss_D: -4.7304 Loss_G: 9.9906\n","Epoch 70\n","[71/70][1/4] Loss_D: -3.5300 Loss_G: 0.0533\n","[71/70][2/4] Loss_D: -3.5350 Loss_G: 12.4187\n","[71/70][3/4] Loss_D: -3.5038 Loss_G: 14.9999\n","[71/70][4/4] Loss_D: -2.4529 Loss_G: 0.3309\n","Epoch 71\n","[72/70][1/4] Loss_D: -3.4319 Loss_G: 3.8379\n","[72/70][2/4] Loss_D: -3.5014 Loss_G: 7.7679\n","[72/70][3/4] Loss_D: -3.2082 Loss_G: 19.3227\n","[72/70][4/4] Loss_D: -2.8876 Loss_G: 2.1618\n","Epoch 72\n","[73/70][1/4] Loss_D: -3.3814 Loss_G: -0.5618\n","[73/70][2/4] Loss_D: -3.9603 Loss_G: 12.5844\n","[73/70][3/4] Loss_D: -1.7928 Loss_G: 8.1279\n","[73/70][4/4] Loss_D: -3.5939 Loss_G: -3.5100\n","Epoch 73\n","[74/70][1/4] Loss_D: -3.1549 Loss_G: 5.3262\n","[74/70][2/4] Loss_D: -2.4898 Loss_G: 14.2957\n","[74/70][3/4] Loss_D: -3.9235 Loss_G: 16.0241\n","[74/70][4/4] Loss_D: -3.3451 Loss_G: 9.4784\n","Epoch 74\n","[75/70][1/4] Loss_D: -3.9115 Loss_G: 5.9006\n","[75/70][2/4] Loss_D: -2.8371 Loss_G: -1.6234\n","[75/70][3/4] Loss_D: -3.5741 Loss_G: 19.1531\n","[75/70][4/4] Loss_D: -2.6143 Loss_G: 5.0945\n","Epoch 75\n","[76/70][1/4] Loss_D: -3.3110 Loss_G: -5.6209\n","[76/70][2/4] Loss_D: -2.5091 Loss_G: 6.1209\n","[76/70][3/4] Loss_D: -4.0522 Loss_G: 13.6864\n","[76/70][4/4] Loss_D: -2.5504 Loss_G: 22.4762\n","Epoch 76\n","[77/70][1/4] Loss_D: -2.9722 Loss_G: 0.2814\n","[77/70][2/4] Loss_D: -1.5549 Loss_G: 13.7636\n","[77/70][3/4] Loss_D: -3.5600 Loss_G: 12.5006\n","[77/70][4/4] Loss_D: -3.3388 Loss_G: 15.3505\n","Epoch 77\n","[78/70][1/4] Loss_D: -2.7787 Loss_G: 6.1805\n","[78/70][2/4] Loss_D: -3.8514 Loss_G: -10.8321\n","[78/70][3/4] Loss_D: -3.9490 Loss_G: 12.7217\n","[78/70][4/4] Loss_D: -2.7220 Loss_G: 20.6204\n","Epoch 78\n","[79/70][1/4] Loss_D: -2.6226 Loss_G: 15.8920\n","[79/70][2/4] Loss_D: -3.9103 Loss_G: 1.6691\n","[79/70][3/4] Loss_D: -2.9468 Loss_G: 1.6069\n","[79/70][4/4] Loss_D: -4.1916 Loss_G: 13.9996\n","Epoch 79\n","[80/70][1/4] Loss_D: -1.9972 Loss_G: 18.5312\n","[80/70][2/4] Loss_D: -2.7862 Loss_G: 0.0173\n","[80/70][3/4] Loss_D: -3.1483 Loss_G: 2.0888\n","[80/70][4/4] Loss_D: -3.0655 Loss_G: 11.1462\n","Epoch 80\n","[81/70][1/4] Loss_D: -3.5867 Loss_G: 6.3890\n","[81/70][2/4] Loss_D: -3.0501 Loss_G: -1.4342\n","[81/70][3/4] Loss_D: -2.0954 Loss_G: 2.5286\n","[81/70][4/4] Loss_D: -4.4763 Loss_G: 18.0415\n","Epoch 81\n","[82/70][1/4] Loss_D: -3.8487 Loss_G: 4.2123\n","[82/70][2/4] Loss_D: -2.0378 Loss_G: -3.7083\n","[82/70][3/4] Loss_D: -3.3162 Loss_G: 13.3226\n","[82/70][4/4] Loss_D: -2.1805 Loss_G: 14.3277\n","Epoch 82\n","[83/70][1/4] Loss_D: -2.3068 Loss_G: 9.9380\n","[83/70][2/4] Loss_D: -3.3855 Loss_G: 8.6815\n","[83/70][3/4] Loss_D: -2.7347 Loss_G: -2.8139\n","[83/70][4/4] Loss_D: -1.5996 Loss_G: 16.1850\n","Epoch 83\n","[84/70][1/4] Loss_D: -3.3770 Loss_G: 11.8446\n","[84/70][2/4] Loss_D: -2.8977 Loss_G: 8.8087\n","[84/70][3/4] Loss_D: -2.2799 Loss_G: 13.8867\n","[84/70][4/4] Loss_D: -3.2728 Loss_G: 6.5900\n","Epoch 84\n","[85/70][1/4] Loss_D: -2.6947 Loss_G: 14.2889\n","[85/70][2/4] Loss_D: -3.3147 Loss_G: -4.8141\n","[85/70][3/4] Loss_D: -3.8980 Loss_G: 3.8821\n","[85/70][4/4] Loss_D: -2.5055 Loss_G: 5.7849\n","Epoch 85\n","[86/70][1/4] Loss_D: -1.3731 Loss_G: 13.1598\n","[86/70][2/4] Loss_D: -2.9154 Loss_G: 3.0692\n","[86/70][3/4] Loss_D: -3.9778 Loss_G: -0.6704\n","[86/70][4/4] Loss_D: -3.3070 Loss_G: 6.5692\n","Epoch 86\n","[87/70][1/4] Loss_D: -2.9403 Loss_G: 6.3318\n","[87/70][2/4] Loss_D: -3.1199 Loss_G: 2.9789\n","[87/70][3/4] Loss_D: -2.3954 Loss_G: 20.3154\n","[87/70][4/4] Loss_D: -3.0003 Loss_G: 17.3315\n","Epoch 87\n","[88/70][1/4] Loss_D: -2.5290 Loss_G: 15.7914\n","[88/70][2/4] Loss_D: -2.2660 Loss_G: -5.3984\n","[88/70][3/4] Loss_D: -2.7169 Loss_G: 13.2632\n","[88/70][4/4] Loss_D: -2.5839 Loss_G: 18.0169\n","Epoch 88\n","[89/70][1/4] Loss_D: -2.4705 Loss_G: 0.5121\n","[89/70][2/4] Loss_D: -2.4680 Loss_G: 17.0386\n","[89/70][3/4] Loss_D: -3.6831 Loss_G: 15.3196\n","[89/70][4/4] Loss_D: -1.8014 Loss_G: 9.2476\n","Epoch 89\n","[90/70][1/4] Loss_D: -2.6336 Loss_G: 0.5905\n","[90/70][2/4] Loss_D: -4.2443 Loss_G: 14.1097\n","[90/70][3/4] Loss_D: -3.1601 Loss_G: -0.9377\n","[90/70][4/4] Loss_D: -2.9800 Loss_G: -5.7185\n","Epoch 90\n","[91/70][1/4] Loss_D: -3.1978 Loss_G: 16.4234\n","[91/70][2/4] Loss_D: -3.2414 Loss_G: 8.6130\n","[91/70][3/4] Loss_D: -3.9592 Loss_G: 1.4204\n","[91/70][4/4] Loss_D: -4.0995 Loss_G: 2.7132\n","Epoch 91\n","[92/70][1/4] Loss_D: -2.9877 Loss_G: 5.2260\n","[92/70][2/4] Loss_D: -3.0711 Loss_G: 0.9107\n","[92/70][3/4] Loss_D: -3.8848 Loss_G: 11.9964\n","[92/70][4/4] Loss_D: -3.2234 Loss_G: -4.7236\n","Epoch 92\n","[93/70][1/4] Loss_D: -3.0592 Loss_G: -3.0096\n","[93/70][2/4] Loss_D: -2.6534 Loss_G: -3.7911\n","[93/70][3/4] Loss_D: -3.2068 Loss_G: 11.4721\n","[93/70][4/4] Loss_D: -3.4708 Loss_G: 8.3282\n","Epoch 93\n","[94/70][1/4] Loss_D: -3.8812 Loss_G: 1.5727\n","[94/70][2/4] Loss_D: -3.7157 Loss_G: 8.2981\n","[94/70][3/4] Loss_D: -2.9037 Loss_G: 16.3388\n","[94/70][4/4] Loss_D: -3.3445 Loss_G: 8.6344\n","Epoch 94\n","[95/70][1/4] Loss_D: -4.3579 Loss_G: 11.5091\n","[95/70][2/4] Loss_D: -2.9645 Loss_G: 2.6321\n","[95/70][3/4] Loss_D: -2.8221 Loss_G: 11.2126\n","[95/70][4/4] Loss_D: -2.8688 Loss_G: 22.5533\n","Epoch 95\n","[96/70][1/4] Loss_D: -2.9673 Loss_G: 11.5244\n","[96/70][2/4] Loss_D: -4.0604 Loss_G: 5.0214\n","[96/70][3/4] Loss_D: -3.0835 Loss_G: 4.6580\n","[96/70][4/4] Loss_D: -3.7189 Loss_G: 7.2000\n","Epoch 96\n","[97/70][1/4] Loss_D: -2.5136 Loss_G: 12.3892\n","[97/70][2/4] Loss_D: -3.3633 Loss_G: 11.9341\n","[97/70][3/4] Loss_D: -2.4669 Loss_G: 23.5647\n","[97/70][4/4] Loss_D: -1.8153 Loss_G: 6.1968\n","Epoch 97\n","[98/70][1/4] Loss_D: -3.6720 Loss_G: 0.7387\n","[98/70][2/4] Loss_D: -2.9101 Loss_G: 19.8405\n","[98/70][3/4] Loss_D: -2.6292 Loss_G: 6.7419\n","[98/70][4/4] Loss_D: -3.4603 Loss_G: 14.1993\n","Epoch 98\n","[99/70][1/4] Loss_D: -3.2114 Loss_G: -2.5546\n","[99/70][2/4] Loss_D: -2.8453 Loss_G: 1.2278\n","[99/70][3/4] Loss_D: -2.9861 Loss_G: 19.6806\n","[99/70][4/4] Loss_D: -3.1435 Loss_G: 11.6207\n","Epoch 99\n","[100/70][1/4] Loss_D: -2.4520 Loss_G: 7.5603\n","[100/70][2/4] Loss_D: -3.2540 Loss_G: 8.0781\n","[100/70][3/4] Loss_D: -2.8656 Loss_G: 0.1560\n","[100/70][4/4] Loss_D: -2.0926 Loss_G: 12.0333\n"]}]},{"cell_type":"code","source":["# Guardar el modelo en un archivo .pth\n","gen_path = \"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v1_Images/models/generator.pth\"\n","dis_path = \"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v1_Images/models/discriminator.pth\"\n","\n","torch.save(generator.state_dict(), gen_path)\n","torch.save(discriminator.state_dict(), dis_path)\n","\n","# Función para generar una imagen aleatoria y guardarla en un archivo .jpg\n","def generar_imagen_aleatoria(generator_path, output_path):\n","    # Cargar los pesos del generador desde el archivo .pth\n","    generator = Generator()  # Reemplaza \"Generator()\" con la clase o función que define tu generador\n","    generator.load_state_dict(torch.load(generator_path))\n","    generator.eval()\n","\n","    # Generar una imagen aleatoria\n","    with torch.no_grad():\n","        noise = torch.randn(1, 128, 1, 1)  # Ajusta el tamaño del ruido según tu generador\n","        imagen_generada = generator(noise)\n","\n","    # Guardar la imagen generada en un archivo .jpg\n","    vutils.save_image(imagen_generada, output_path, normalize=True)\n"],"metadata":{"id":"aNfVHFEbQHSN","executionInfo":{"status":"ok","timestamp":1687682869638,"user_tz":-120,"elapsed":2,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Ejemplo de uso:\n","output_path = \"/content/gdrive/MyDrive/Dev/AI_MsC/TFM/BestNotebooks/wGAN-v1_Images/models/output.jpg\"\n","generar_imagen_aleatoria(gen_path, output_path)"],"metadata":{"id":"QKgFDLfGt1c0","executionInfo":{"status":"ok","timestamp":1687682872839,"user_tz":-120,"elapsed":375,"user":{"displayName":"Pablo Bolívar","userId":"01242547030775017581"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"AEwMELE4QI0L"},"execution_count":null,"outputs":[]}]}